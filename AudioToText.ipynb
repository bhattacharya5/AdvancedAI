{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharya5/AdvancedAI/blob/main/AudioToText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLijULPUX-wT",
        "outputId": "548a3eaa-0487-476a-a3d7-d57ef407c389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-kn5l7u6a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-kn5l7u6a\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=864381d7f41e9f79df2607718462b1054e39e9d6227c4a4706aed97eab2db768\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vv_j3tfd/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfkAupJxYIOe",
        "outputId": "a456d622-57f8-4e9d-8d93-07bf1d296a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Waiting for headers] [1 I\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Waiting for headers] [Con\u001b[0m\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [2 InRelease 14.2 kB/110 kB 13%] [Connecting to ppa.la\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [2 InRelease 14.2 kB/110 kB 13%] [Connecting to ppa.la\u001b[0m\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [662 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,056 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,320 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,624 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\n",
            "Fetched 5,084 kB in 2s (2,830 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ]
        }
      ],
      "source": [
        " !sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXLxpNv5vJOe",
        "outputId": "fbab43e2-6b2d-46ce-fd2b-c969295dc277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91671823\n",
            "os.stat_result(st_mode=33188, st_ino=1441793, st_dev=52, st_nlink=1, st_uid=0, st_gid=0, st_size=91671823, st_atime=1705820123, st_mtime=1705820228, st_ctime=1705820228)\n",
            "File Size in Bytes is 91671823\n",
            "File Size in MegaBytes is 87.42506313323975\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.getsize(\"class4.mp3\"))\n",
        "file_stats = os.stat(\"class4.mp3\")\n",
        "\n",
        "\n",
        "print(file_stats)\n",
        "print(f'File Size in Bytes is {file_stats.st_size}')\n",
        "print(f'File Size in MegaBytes is {file_stats.st_size / (1024 * 1024)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10fM1d9pYw8F",
        "outputId": "f703e2c8-d109-4a71-a6f2-6ec992f404e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:09<00:00, 158MiB/s]\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:00.000 --> 00:16.080]  Okay, so last time we had a slide on this slide, okay, I'm audible, right, clearly.\n",
            "[00:16.080 --> 00:17.080]  Yes, sir.\n",
            "[00:17.080 --> 00:18.080]  Yes, sir.\n",
            "[00:18.080 --> 00:20.760]  So let me revise this slide.\n",
            "[00:20.760 --> 00:25.280]  So basically, the idea of high boost filtering, as I was saying, is that you take the input\n",
            "[00:25.280 --> 00:31.840]  signal, you blur that signal, instead of a 1D signal in the image processing, you have\n",
            "[00:31.840 --> 00:32.840]  an image.\n",
            "[00:32.840 --> 00:38.680]  So you blur that image with Gaussian or whatever averaging filter, right, then essentially\n",
            "[00:38.680 --> 00:43.560]  what you do, so this you can one possible way to extract the high frequency details\n",
            "[00:43.560 --> 00:48.680]  is subtract the blurred image from the input signal.\n",
            "[00:48.680 --> 00:51.800]  Okay, now this is one way to do it.\n",
            "[00:51.800 --> 00:58.320]  The other way to do it is what I mentioned last time, which is you compute the Laplacian\n",
            "[00:58.320 --> 01:00.360]  of the signal.\n",
            "[01:00.360 --> 01:01.360]  Okay.\n",
            "[01:01.360 --> 01:08.440]  Now, essentially, here, what is explained in the formula below is f bar is the blurred\n",
            "[01:08.440 --> 01:13.520]  image, f is the original input image.\n",
            "[01:13.520 --> 01:19.120]  If you'll subtract this blurred image from the input image, you will get this high frequency\n",
            "[01:19.800 --> 01:20.800]  mask, G mask.\n",
            "[01:20.800 --> 01:27.520]  Again, now this mask is not, this is a full image, okay, this is not a 3x3 or 5x5 filter.\n",
            "[01:27.520 --> 01:28.520]  This is a full image.\n",
            "[01:28.520 --> 01:36.440]  If f is m x n pixels, f bar will also be m x n pixels and G mask will also be m x n pixels.\n",
            "[01:36.440 --> 01:44.240]  Now this particular image, right, which is nothing but the high frequency details are\n",
            "[01:44.240 --> 01:45.240]  there.\n",
            "[01:45.240 --> 01:51.160]  This I will add to original image after multiplying to some k constant, right.\n",
            "[01:51.160 --> 01:57.080]  So this is essentially very similar to what we have seen in the Laplacian based enhancement.\n",
            "[01:57.080 --> 02:03.240]  In fact, the book Gonzalves and Wood that I was referring to till now has this one particular\n",
            "[02:03.240 --> 02:09.560]  implementation of high boost filtering where they are using Laplacian mask, okay, or the\n",
            "[02:09.560 --> 02:12.480]  Laplacian output, Laplacian edge map.\n",
            "[02:13.040 --> 02:19.960]  So essentially this formula at the bottom, this formulation, all it's saying is input\n",
            "[02:19.960 --> 02:24.760]  image is added with edges and these edges are nothing but high frequency details that\n",
            "[02:24.760 --> 02:31.360]  are obtained by subtracting the blurred image from the original image, right.\n",
            "[02:31.360 --> 02:36.840]  So when I blur it, I'm losing information around the edges and this difference is what\n",
            "[02:36.840 --> 02:42.960]  is captured here as these two lobes of inverted lobe of caution, kind of bell lobes, right.\n",
            "[02:42.960 --> 02:49.360]  And essentially when I add these three, I multiply this by lambda, lambda could be anything\n",
            "[02:49.360 --> 02:55.480]  or k and then I actually add it to the original signal one that gives me the enhanced signal\n",
            "[02:55.480 --> 03:02.280]  where the edge boundaries are more emphasized, right.\n",
            "[03:02.280 --> 03:03.620]  And this is the effect we get.\n",
            "[03:03.620 --> 03:06.120]  So this is the original input image on the left.\n",
            "[03:06.120 --> 03:09.660]  On the top is a blurred version of this image, which is F bar.\n",
            "[03:09.660 --> 03:14.360]  When I subtract from the input image is blurred image, what I got is on the bottom.\n",
            "[03:14.360 --> 03:19.700]  And if I get this bottom image multiplied by some k and add to the original image, I\n",
            "[03:19.700 --> 03:23.820]  get this on the right side, right.\n",
            "[03:23.820 --> 03:29.420]  So from that previous graph here, one, two, three, four, this is one, this is two, right.\n",
            "[03:29.420 --> 03:35.020]  This is three and this is four.\n",
            "[03:35.020 --> 03:38.020]  Is this clear to everyone?\n",
            "[03:38.020 --> 03:39.020]  Yes.\n",
            "[03:39.020 --> 03:40.020]  Okay.\n",
            "[03:40.020 --> 03:49.700]  And in the book, if you really look at the version that I have second edition, they clearly\n",
            "[03:49.700 --> 03:54.940]  say that, okay, this is a way to, this is a way to manifest the high boost filtering.\n",
            "[03:54.940 --> 04:01.620]  But many times we simply take the Laplacian outcome, which is the Laplacian mask applied\n",
            "[04:01.620 --> 04:03.060]  to the image.\n",
            "[04:03.060 --> 04:11.220]  And that we basically use instead of this simple subtraction of the blurred image from\n",
            "[04:11.220 --> 04:12.940]  the original image to get the edge map.\n",
            "[04:12.940 --> 04:18.900]  So there was no confusion last time also, what I was teaching was consistent, right.\n",
            "[04:18.900 --> 04:22.740]  But that was one way to implement high boost filtering.\n",
            "[04:22.740 --> 04:26.540]  But here, the simpler way, which is explained here in the formula below is you take the\n",
            "[04:26.540 --> 04:31.020]  input image, you blur it, subtract the blurred image from the input image.\n",
            "[04:31.020 --> 04:35.700]  And then that edge map that you get, we call here as G mask.\n",
            "[04:35.700 --> 04:39.780]  This you multiply it by some k constant and add to the original image.\n",
            "[04:39.780 --> 04:45.940]  So this will emphasize, enhance the edge regions of the image and make it more sharper as shown\n",
            "[04:45.940 --> 04:51.160]  here or here, right.\n",
            "[04:51.160 --> 04:55.440]  And then we talked about this Robert Cross and Sebel gradient, we discussed about what\n",
            "[04:55.440 --> 05:01.640]  kind of edges we can extract more focused on vertical, horizontal or diagonal edges.\n",
            "[05:01.640 --> 05:05.400]  We also looked at some non-linear filters for noise removal.\n",
            "[05:05.400 --> 05:09.080]  For example, in this case, it was the pepper noise.\n",
            "[05:09.080 --> 05:12.080]  So we can use max filter.\n",
            "[05:12.080 --> 05:14.320]  If it is a salt noise, we can use min filter.\n",
            "[05:14.320 --> 05:21.140]  If it is salt and pepper together, then we have, we should use median filter, right.\n",
            "[05:22.120 --> 05:23.380]  So till here, we talked about in the last class.\n",
            "[05:23.380 --> 05:29.000]  Now we will start with the new topic today, which is bilateral filtering.\n",
            "[05:29.000 --> 05:35.700]  And this filtering is nowadays very popular with most of the smartphones they use for\n",
            "[05:35.700 --> 05:36.700]  portrait images.\n",
            "[05:36.700 --> 05:43.300]  They use this kind of bilateral filtering, which in a way also is interpreted by many\n",
            "[05:43.300 --> 05:45.420]  as a beautification feature.\n",
            "[05:45.700 --> 05:52.260]  And the idea is that the fine-grained details of the skin when you have texture or because\n",
            "[05:52.260 --> 06:00.740]  of the very small micro level, you can say irregularities of the surface of the skin\n",
            "[06:00.740 --> 06:06.460]  of the face, those details can be blurred out while retaining the high-frequency details\n",
            "[06:06.460 --> 06:13.740]  of the facial details like eyelids and their boundaries and the sharpness of the nose and\n",
            "[06:14.060 --> 06:17.380]  the lip region, all of them intact.\n",
            "[06:17.380 --> 06:27.220]  So the idea is if I just do a Gaussian blurring to hide those irregularities or details of\n",
            "[06:27.220 --> 06:30.580]  the facial region, what I will get is the middle image.\n",
            "[06:30.580 --> 06:34.620]  So left image is the input image, middle is the blurred image.\n",
            "[06:34.620 --> 06:41.260]  But what I am basically looking for is a right image where I am retaining all the sharp details,\n",
            "[06:41.780 --> 06:46.940]  like the reflection in the cornea of the eye, I want to retain that.\n",
            "[06:46.940 --> 06:53.660]  I want to retain the fineness of the details that are out there in the facial region and\n",
            "[06:53.660 --> 06:55.180]  lip region and all.\n",
            "[06:55.180 --> 07:00.900]  But I want to get away with the details of the cheek area, of the forehead area.\n",
            "[07:00.900 --> 07:06.780]  I want to suppress those details, those high-frequency details.\n",
            "[07:07.300 --> 07:15.540]  So this is the key aim of bilateral filtering and you can see how it basically suppresses\n",
            "[07:15.540 --> 07:24.380]  the many details which unfortunately in society are considered to be not a beautiful thing.\n",
            "[07:24.380 --> 07:30.740]  But this is the way things are for the bilateral filtering, so it tries to suppress the high-frequency\n",
            "[07:30.740 --> 07:36.020]  details while retaining the, sorry, tries to suppress the high-frequency details in\n",
            "[07:36.020 --> 07:40.540]  certain area of the image while retaining the details in the other area of the image.\n",
            "[07:40.540 --> 07:42.140]  So how do we do that?\n",
            "[07:42.140 --> 07:46.860]  So we need to have either a way to figure out what are the regions from where I have\n",
            "[07:46.860 --> 07:53.700]  to blur out or I need some natural way to figure out that where are the edges and those\n",
            "[07:53.700 --> 07:59.140]  edges I would like to retain but everywhere else where I have homogeneous region, I want\n",
            "[07:59.140 --> 08:02.140]  to average things out.\n",
            "[08:02.140 --> 08:03.140]  Okay.\n",
            "[08:03.140 --> 08:04.140]  Hi sir.\n",
            "[08:04.140 --> 08:05.140]  Yeah.\n",
            "[08:05.140 --> 08:09.140]  Some of the students actually want to enter the class.\n",
            "[08:09.140 --> 08:12.140]  Can you please admit again?\n",
            "[08:12.140 --> 08:13.140]  Sorry to interrupt.\n",
            "[08:13.140 --> 08:14.140]  No, no.\n",
            "[08:14.140 --> 08:15.140]  Okay.\n",
            "[08:15.140 --> 08:16.140]  But I can't see them in the lobby.\n",
            "[08:16.140 --> 08:17.140]  Oh.\n",
            "[08:17.140 --> 08:18.140]  Okay.\n",
            "[08:18.140 --> 08:19.140]  Okay.\n",
            "[08:19.140 --> 08:20.140]  All.\n",
            "[08:20.140 --> 08:23.140]  How do I see the lobby?\n",
            "[08:23.140 --> 08:26.140]  I am not.\n",
            "[08:26.140 --> 08:29.140]  Just click on the participants or so.\n",
            "[08:29.140 --> 08:30.140]  Yeah.\n",
            "[08:30.140 --> 08:33.140]  I am in the participant, right?\n",
            "[08:33.140 --> 08:34.140]  People.\n",
            "[08:34.140 --> 08:38.140]  Someone wants to go in the lobby view.\n",
            "[08:38.140 --> 08:39.140]  Okay.\n",
            "[08:39.140 --> 08:40.140]  Here.\n",
            "[08:40.140 --> 08:41.140]  Admit all.\n",
            "[08:41.140 --> 08:42.140]  This one guy.\n",
            "[08:42.140 --> 08:43.140]  Okay.\n",
            "[08:43.140 --> 08:46.140]  Guys, please try to join on time.\n",
            "[08:46.140 --> 08:47.140]  Okay.\n",
            "[08:47.140 --> 08:49.140]  Otherwise we get this kind of interruption.\n",
            "[08:49.140 --> 08:51.140]  Somebody sharing the YouTube link and all.\n",
            "[08:51.140 --> 08:52.140]  What is that in the chat?\n",
            "[08:52.140 --> 08:53.140]  If anything is there.\n",
            "[08:53.140 --> 08:55.140]  I have shared this link for that.\n",
            "[08:55.140 --> 08:57.140]  You can use your iPad as a link.\n",
            "[08:57.140 --> 08:59.140]  Probably later you can use it.\n",
            "[08:59.140 --> 09:00.140]  Okay.\n",
            "[09:00.140 --> 09:02.140]  Let me try to fix that.\n",
            "[09:02.140 --> 09:03.140]  Okay.\n",
            "[09:03.140 --> 09:04.140]  Thank you.\n",
            "[09:04.140 --> 09:07.140]  So then how do we achieve this?\n",
            "[09:07.140 --> 09:13.140]  So one way to look at it, if I simply take the Gaussian filter and apply it on all possible\n",
            "[09:13.140 --> 09:19.140]  location of the image, which is this slide is showing essentially if the image region\n",
            "[09:19.140 --> 09:25.140]  on the top, what you're seeing here is the flat area with pretty much same appearance,\n",
            "[09:25.140 --> 09:26.140]  right?\n",
            "[09:26.140 --> 09:28.140]  Then it will be blurred out somewhere here.\n",
            "[09:28.140 --> 09:30.140]  You can see here in this area.\n",
            "[09:30.140 --> 09:37.140]  If I really take an edge region and if I apply a blurring, if you see here, the edges are\n",
            "[09:37.140 --> 09:38.140]  blurred out.\n",
            "[09:38.140 --> 09:39.140]  Okay.\n",
            "[09:39.140 --> 09:45.140]  If I take this region, which has a lot of details, this is a zoomed in version of that window.\n",
            "[09:45.140 --> 09:50.140]  If I apply this kind of uniform blurring, then also these details will be blurred out.\n",
            "[09:50.140 --> 09:57.140]  So Gaussian filter does not take into consideration the local appearance variation, right?\n",
            "[09:57.140 --> 09:58.140]  For every window.\n",
            "[09:58.140 --> 10:01.140]  All it does it, it just do averaging, right?\n",
            "[10:01.140 --> 10:02.140]  Centered at a pixel.\n",
            "[10:02.140 --> 10:08.140]  It does just that the weighted averaging of all neighbors and that blur out details at\n",
            "[10:08.140 --> 10:11.140]  all regions of the image, right?\n",
            "[10:11.140 --> 10:15.140]  But what we want to do is we want to identify the flat regions.\n",
            "[10:15.140 --> 10:17.140]  There I'll apply simple Gaussian blurring.\n",
            "[10:17.140 --> 10:20.140]  Wherever I edge, I will have an edge.\n",
            "[10:20.140 --> 10:23.140]  Then I will not blur out the edge region, right?\n",
            "[10:23.140 --> 10:28.140]  So to be able to do that, what I need is this kind of filter mask.\n",
            "[10:28.140 --> 10:33.140]  So the flat region on the top, this top window here, I will apply simple Gaussian.\n",
            "[10:33.140 --> 10:40.140]  But on this region where I have an edge on this rock and the background sky, I will basically\n",
            "[10:40.140 --> 10:41.140]  apply this kind of filter.\n",
            "[10:41.140 --> 10:46.140]  So it will blur out the sky region, but it will not blur out the details in the edge\n",
            "[10:46.140 --> 10:47.140]  region.\n",
            "[10:47.140 --> 10:48.140]  Okay.\n",
            "[10:48.140 --> 10:52.140]  And then again, here, if you really look at it.\n",
            "[10:52.140 --> 10:56.140]  In this area, there's a lot of detail, right?\n",
            "[10:56.140 --> 11:01.140]  It, okay, the slide got changed.\n",
            "[11:01.140 --> 11:03.140]  So in this area, there's a lot of details.\n",
            "[11:03.140 --> 11:06.140]  It will try to, if you see here, right?\n",
            "[11:06.140 --> 11:11.140]  It will try to add wherever there's a flat area, it will blur.\n",
            "[11:11.140 --> 11:14.140]  Wherever there are details, it will not blur.\n",
            "[11:14.140 --> 11:15.140]  Okay.\n",
            "[11:15.140 --> 11:16.140]  So these details here.\n",
            "[11:16.140 --> 11:21.140]  So again, the idea is that can I design a filter which is aware of the appearance of\n",
            "[11:21.140 --> 11:22.140]  the patch?\n",
            "[11:22.140 --> 11:23.140]  Right.\n",
            "[11:23.140 --> 11:27.140]  So if the appearance of the patch is flat, then it will do simple Gaussian blurring.\n",
            "[11:27.140 --> 11:32.140]  If the appearance of the patch is not flat, then it will not blur out those regions which\n",
            "[11:32.140 --> 11:34.140]  are part of the edge.\n",
            "[11:34.140 --> 11:35.140]  Okay.\n",
            "[11:35.140 --> 11:39.140]  So essentially, this is the formula of bilateral filtering.\n",
            "[11:39.140 --> 11:44.140]  Now here, just look at this formula, what I'm doing.\n",
            "[11:44.140 --> 11:47.140]  So I is the filtered image output.\n",
            "[11:47.140 --> 11:48.140]  Okay.\n",
            "[11:48.140 --> 11:52.140]  X is a particular location where I'm applying the filter mask, special filter.\n",
            "[11:52.140 --> 11:54.140]  I is the input image.\n",
            "[11:54.140 --> 11:55.140]  Right.\n",
            "[11:55.140 --> 12:00.140]  X is, as I said, is a coordinate of the current pixel along around which I'm applying the\n",
            "[12:00.140 --> 12:02.140]  neighborhood filter mask.\n",
            "[12:02.140 --> 12:04.140]  Omega is that neighborhood window.\n",
            "[12:04.140 --> 12:09.140]  So X is any pixel in the neighborhood of X.\n",
            "[12:09.140 --> 12:10.140]  Okay.\n",
            "[12:10.140 --> 12:11.140]  So this is what I mean.\n",
            "[12:11.140 --> 12:17.140]  So for all some or all pixels, X is which are neighbors of X.\n",
            "[12:17.140 --> 12:19.140]  So Omega is a neighborhood.\n",
            "[12:19.140 --> 12:20.140]  Okay.\n",
            "[12:20.140 --> 12:21.140]  Set.\n",
            "[12:21.140 --> 12:27.140]  And FR and GS are two kernels or two functions that I'm applying to smoothen out.\n",
            "[12:27.140 --> 12:28.140]  Right.\n",
            "[12:28.140 --> 12:30.140]  So GS is a Gaussian function.\n",
            "[12:30.140 --> 12:31.140]  Right.\n",
            "[12:31.140 --> 12:38.140]  So essentially what GS does, it basically look at the influence of neighboring pixels\n",
            "[12:38.140 --> 12:40.140]  is more when I'm doing averaging.\n",
            "[12:40.140 --> 12:41.140]  So all I'm doing is averaging.\n",
            "[12:41.140 --> 12:44.140]  I'm doing some or all neighbors, I of XI.\n",
            "[12:44.140 --> 12:47.140]  So I'm taking intensity of all neighbors.\n",
            "[12:47.140 --> 12:48.140]  Right.\n",
            "[12:48.140 --> 12:53.140]  If these two terms are not there, essentially it is simply an averaging some of all elements,\n",
            "[12:53.140 --> 12:55.140]  which is the average filter.\n",
            "[12:55.140 --> 12:56.140]  Right.\n",
            "[12:56.140 --> 12:57.140]  What was the average filter?\n",
            "[12:57.140 --> 12:58.140]  Right.\n",
            "[12:58.140 --> 13:06.140]  For a three by three neighborhood, I will take all elements in that three by three neighborhood,\n",
            "[13:06.140 --> 13:11.140]  take the intensity value of them, sum it, divide by nine, and that will be my blurred\n",
            "[13:11.140 --> 13:13.140]  image output at that pixel.\n",
            "[13:13.140 --> 13:14.140]  Right.\n",
            "[13:14.140 --> 13:21.140]  Now, what I'm doing here is when I just forget about FR here, if I just considered as GS,\n",
            "[13:21.140 --> 13:29.140]  then essentially I am simply saying that if the pixel XI is far away from X, then its\n",
            "[13:29.140 --> 13:33.140]  contribution to the averaging should be low because this is a Gaussian kernel.\n",
            "[13:33.140 --> 13:40.140]  The Gaussian kernel or Gaussian function is e raised to the power minus XI minus X and\n",
            "[13:40.140 --> 13:46.140]  some other normalizing term and variance that literally means if XI and X is a distance,\n",
            "[13:46.140 --> 13:56.140]  if that distance is larger, then e raised to the power minus XI minus X will be smaller.\n",
            "[13:56.140 --> 13:57.140]  Right.\n",
            "[13:57.140 --> 14:02.140]  So you have to really look at and let me show you the Gaussian curve for that if possible,\n",
            "[14:02.140 --> 14:09.140]  exponential curve, because I really wanted to draw things today and I'm not able to do that.\n",
            "[14:10.140 --> 14:17.140]  Let me see if I can draw it here.\n",
            "[14:17.140 --> 14:19.140]  You want to see the screen, right?\n",
            "[14:19.140 --> 14:22.140]  First of all, let me share the screen with you.\n",
            "[14:22.140 --> 14:26.140]  Now I have to see if I can insert draw here.\n",
            "[14:26.140 --> 14:28.140]  Let me draw.\n",
            "[14:28.140 --> 14:34.140]  So if I really look at this is an axis X and Y.\n",
            "[14:34.140 --> 14:38.140]  Now let's look at a Gaussian, an exponential curve.\n",
            "[14:38.140 --> 14:40.140]  How does it look?\n",
            "[14:40.140 --> 14:41.140]  Right.\n",
            "[14:41.140 --> 14:47.140]  So this is how an exponential curve look like.\n",
            "[14:47.140 --> 14:48.140]  Right.\n",
            "[14:48.140 --> 14:53.140]  So essentially as X increases, Y increases exponentially.\n",
            "[14:53.140 --> 14:58.140]  This is my X and this is my Y.\n",
            "[14:58.140 --> 15:00.140]  Right. This is an exponential curve.\n",
            "[15:00.140 --> 15:01.140]  Right.\n",
            "[15:01.140 --> 15:04.140]  What will be the inverse exponential curve will do?\n",
            "[15:04.140 --> 15:09.140]  So an inverse exponential curve, if I may draw it with another color.\n",
            "[15:15.140 --> 15:16.140]  Right.\n",
            "[15:16.140 --> 15:19.140]  This is a close to an inverse exponential curve.\n",
            "[15:19.140 --> 15:25.140]  So this is e raised to the power X.\n",
            "[15:25.140 --> 15:35.140]  And this e raised to the power minus X is the variant of Gaussian.\n",
            "[15:35.140 --> 15:41.140]  If I take instead of X, if I say X i minus X, then essentially this is the inverse Gaussian.\n",
            "[15:41.140 --> 15:42.140]  Okay.\n",
            "[15:42.140 --> 15:43.140]  With appropriate scaling.\n",
            "[15:43.140 --> 15:54.140]  Now what you are essentially seeing here is that if this term in the G of S, if this term is large, then the Y value of this is small.\n",
            "[15:54.140 --> 15:55.140]  Right.\n",
            "[15:55.140 --> 15:59.140]  So Y is also called sometime affinity or influence.\n",
            "[15:59.140 --> 16:09.140]  So essentially if a pixel is far away from the center pixel, then its influence or its importance is less with respect to appearance of that pixel.\n",
            "[16:09.140 --> 16:11.140]  That is the idea of Gaussian blurring.\n",
            "[16:11.140 --> 16:12.140]  Right.\n",
            "[16:12.140 --> 16:17.140]  So essentially neighboring pixels will give more importance when I am doing averaging.\n",
            "[16:17.140 --> 16:18.140]  So this is some kind of weight.\n",
            "[16:18.140 --> 16:23.140]  So I'm actually computing some kind of Y axis turns out to be weight for the gray pixel.\n",
            "[16:23.140 --> 16:24.140]  The blue.\n",
            "[16:24.140 --> 16:25.140]  Right.\n",
            "[16:25.140 --> 16:28.140]  This is a weight of the kernel or the mask.\n",
            "[16:28.140 --> 16:29.140]  Okay.\n",
            "[16:29.140 --> 16:38.140]  Now, essentially, and this is my I can write it as X i minus X.\n",
            "[16:38.140 --> 16:39.140]  Right.\n",
            "[16:39.140 --> 16:51.140]  So essentially what is happening is when I do this, I am saying that far away pixels should have less impact on the appearance of the center pixel when I'm doing averaging.\n",
            "[16:51.140 --> 16:52.140]  Right.\n",
            "[16:52.140 --> 16:55.140]  So this is a standard Gaussian filtering formula.\n",
            "[16:55.140 --> 17:03.140]  If I don't look at this part, if I remove this and underlying that I have put here right in the box here.\n",
            "[17:03.140 --> 17:06.140]  Now, what is this F R is doing?\n",
            "[17:06.140 --> 17:07.140]  Right.\n",
            "[17:07.140 --> 17:09.140]  So what is this F R is doing?\n",
            "[17:09.140 --> 17:11.140]  It could be a Gaussian also.\n",
            "[17:11.140 --> 17:12.140]  It could be any other kernel also.\n",
            "[17:12.140 --> 17:20.140]  It is trying to compute not just the location of the pixel with respect to center pixel, but also the appearance of the pixel.\n",
            "[17:20.140 --> 17:21.140]  Right.\n",
            "[17:21.140 --> 17:25.140]  So even if the pixel is far away, but its appearance is very similar.\n",
            "[17:25.140 --> 17:26.140]  See that gray window.\n",
            "[17:26.140 --> 17:34.140]  If you remember in the gray window with respect to the pixel in the middle, this pixel also has a very similar appearance to this pixel.\n",
            "[17:34.140 --> 17:35.140]  Right.\n",
            "[17:35.140 --> 17:39.140]  So even though it is far, its appearance is similar.\n",
            "[17:39.140 --> 17:40.140]  Right.\n",
            "[17:40.140 --> 17:43.140]  So in that case, what we want to do?\n",
            "[17:43.140 --> 17:46.140]  We want to blur out simply as a Gaussian.\n",
            "[17:46.140 --> 17:47.140]  Right.\n",
            "[17:47.140 --> 17:51.140]  So this value of this F of R should be one in that case.\n",
            "[17:51.140 --> 17:53.140]  Right.\n",
            "[17:53.140 --> 17:56.140]  That means just apply simple Gaussian filtering.\n",
            "[17:56.140 --> 18:09.140]  But if this is different, if I really take this window and if I say appearance of this pixel is gray or light gray and appearance of this pixel,\n",
            "[18:09.140 --> 18:16.140]  let me put that in the red color somewhere here, is the black, even though if you see these are pretty much the same pixels.\n",
            "[18:16.140 --> 18:17.140]  Right.\n",
            "[18:17.140 --> 18:19.140]  But now its appearance is different.\n",
            "[18:19.140 --> 18:27.140]  So now if the appearance is different, then I want to give zero weight or close to zero weight to this function F R.\n",
            "[18:27.140 --> 18:30.140]  That means don't apply the blurring.\n",
            "[18:30.140 --> 18:43.140]  That means this particular pixel which is, which has a certain distance from X, since its appearance is different, its weight should be close to zero.\n",
            "[18:43.140 --> 18:49.140]  That means I should not consider its appearance while doing the averaging of this center pixel here.\n",
            "[18:49.140 --> 18:52.140]  Is that clear?\n",
            "[18:52.140 --> 18:58.140]  So what we are doing is a same pixel at another pixel at the same distance.\n",
            "[18:58.140 --> 19:04.140]  I want to do Gaussian blurring in this area, but I don't want to do Gaussian blurring in this area.\n",
            "[19:04.140 --> 19:07.140]  I don't want it.\n",
            "[19:07.140 --> 19:08.140]  Right.\n",
            "[19:08.140 --> 19:09.140]  Is that clear?\n",
            "[19:09.140 --> 19:18.140]  That basically means I will get this kind of mask, which is Gaussian in this area clearly, but it is zero in this area.\n",
            "[19:18.140 --> 19:27.140]  Sir, the advantage of bilateral filtering is what sir?\n",
            "[19:27.140 --> 19:36.140]  Advantage of bilateral filtering is this, that if you really see here, if I just do Gaussian, output is blurred completely.\n",
            "[19:36.140 --> 19:41.140]  If I do bilateral, if you see the sky, the details are blurred out.\n",
            "[19:41.140 --> 19:43.140]  Even the rock details are blurred out.\n",
            "[19:43.140 --> 19:48.140]  But the edges of this image, they are kept intact.\n",
            "[19:48.140 --> 19:51.140]  They are not blurred out.\n",
            "[19:51.140 --> 19:55.140]  If you compare this versus this.\n",
            "[19:55.140 --> 19:57.140]  Right.\n",
            "[19:57.140 --> 20:01.140]  And why does it advantageous if you really look at this image?\n",
            "[20:01.140 --> 20:02.140]  Right.\n",
            "[20:02.140 --> 20:04.140]  Or maybe this image is much more blurred out.\n",
            "[20:04.140 --> 20:08.140]  You see these details, these details are all lost.\n",
            "[20:08.140 --> 20:10.140]  This is blurred out.\n",
            "[20:10.140 --> 20:11.140]  Right.\n",
            "[20:11.140 --> 20:12.140]  Right.\n",
            "[20:12.140 --> 20:16.140]  But these details, which are the edges, they are not so much blurred out.\n",
            "[20:16.140 --> 20:27.140]  If you really look at the eyebrow, these details are pretty much kept intact as compared to what you see here where they are blurred out.\n",
            "[20:27.140 --> 20:32.140]  Sir, it is not the same to unsharp filtering.\n",
            "[20:32.140 --> 20:40.140]  No, unsharp filtering on they are emphasizing that they are not blurring out the details.\n",
            "[20:40.140 --> 20:41.140]  Okay.\n",
            "[20:41.140 --> 20:44.140]  Unsharp filtering is not blurring out the details.\n",
            "[20:44.140 --> 20:52.140]  I mean, before unsharp filtering, when we were applying that filter.\n",
            "[20:52.140 --> 20:53.140]  Here.\n",
            "[20:53.140 --> 20:57.140]  Before this one, before this filter, there was in the.\n",
            "[20:57.140 --> 20:59.140]  Let me take this example.\n",
            "[20:59.140 --> 21:00.140]  Yes.\n",
            "[21:00.140 --> 21:01.140]  Third one.\n",
            "[21:01.140 --> 21:02.140]  Yes.\n",
            "[21:02.140 --> 21:04.140]  Area is a flat area.\n",
            "[21:04.140 --> 21:05.140]  Right.\n",
            "[21:05.140 --> 21:06.140]  Yes.\n",
            "[21:06.140 --> 21:07.140]  This area was a flat area.\n",
            "[21:07.140 --> 21:08.140]  Nothing has changed.\n",
            "[21:08.140 --> 21:09.140]  Yes.\n",
            "[21:09.140 --> 21:12.140]  Because having some details, those details will significantly change.\n",
            "[21:12.140 --> 21:13.140]  Not necessarily.\n",
            "[21:13.140 --> 21:14.140]  Right.\n",
            "[21:14.140 --> 21:15.140]  Yes.\n",
            "[21:15.140 --> 21:17.140]  Because I am adding the signal.\n",
            "[21:17.140 --> 21:18.140]  The signal is kept intact.\n",
            "[21:18.140 --> 21:21.140]  Only these areas are high.\n",
            "[21:21.140 --> 21:22.140]  Oh, okay.\n",
            "[21:22.140 --> 21:23.140]  Okay.\n",
            "[21:23.140 --> 21:25.140]  So the actual one remains as it is.\n",
            "[21:25.140 --> 21:27.140]  Actual details remain actual.\n",
            "[21:27.140 --> 21:30.140]  I am just adding more fine grained details to it.\n",
            "[21:30.140 --> 21:32.140]  The goal here is opposite.\n",
            "[21:32.140 --> 21:36.140]  The goal here is to blur out certain details.\n",
            "[21:36.140 --> 21:37.140]  Certain details.\n",
            "[21:37.140 --> 21:40.140]  Flat area where intensity is uniform.\n",
            "[21:40.140 --> 21:41.140]  Right.\n",
            "[21:41.140 --> 21:45.140]  While retaining the areas where intensity is uniform.\n",
            "[21:45.140 --> 21:46.140]  Okay.\n",
            "[21:46.140 --> 21:47.140]  Okay.\n",
            "[21:47.140 --> 21:50.140]  Thank you.\n",
            "[21:50.140 --> 21:53.140]  Any other question?\n",
            "[21:53.140 --> 21:55.140]  Sir, I have one question.\n",
            "[21:55.140 --> 21:56.140]  Yes.\n",
            "[21:56.140 --> 21:57.140]  You mentioned that.\n",
            "[21:57.140 --> 22:00.140]  It is mentioned in the slide that sharp.\n",
            "[22:00.140 --> 22:04.140]  The kernel shape is chosen according to the content.\n",
            "[22:04.140 --> 22:05.140]  Okay.\n",
            "[22:05.140 --> 22:06.140]  Image content.\n",
            "[22:06.140 --> 22:07.140]  No.\n",
            "[22:07.140 --> 22:08.140]  Shape is not chosen.\n",
            "[22:08.140 --> 22:09.140]  Kernel type.\n",
            "[22:09.140 --> 22:10.140]  Right.\n",
            "[22:10.140 --> 22:11.140]  Okay.\n",
            "[22:11.140 --> 22:14.140]  So you mean like whether I have to apply Gaussian or this another one.\n",
            "[22:14.140 --> 22:17.140]  That also is not too much an image content.\n",
            "[22:17.140 --> 22:20.140]  Kernel weights are computed on the flyer.\n",
            "[22:20.140 --> 22:22.140]  That's what I say.\n",
            "[22:22.140 --> 22:23.140]  Okay.\n",
            "[22:23.140 --> 22:26.140]  These weights are now dependent on how the intensity is different.\n",
            "[22:26.140 --> 22:27.140]  Right.\n",
            "[22:27.140 --> 22:29.140]  These weights are not fixed.\n",
            "[22:29.140 --> 22:30.140]  Okay.\n",
            "[22:31.140 --> 22:37.140]  Parameters of kernel might be chosen for a fixed value for the whole image.\n",
            "[22:37.140 --> 22:40.140]  Like it's a Gaussian kernel then what is the variance I want to choose.\n",
            "[22:40.140 --> 22:42.140]  All that will impact.\n",
            "[22:42.140 --> 22:43.140]  Okay.\n",
            "[22:43.140 --> 22:50.140]  You might get a different level of blurring if I use Gaussian filter of different variance.\n",
            "[22:50.140 --> 22:51.140]  Okay.\n",
            "[22:51.140 --> 22:53.140]  You might not get so much flattening.\n",
            "[22:53.140 --> 22:56.140]  And this flattening is more visible if I.\n",
            "[22:56.140 --> 22:57.140]  So this is a net effect.\n",
            "[22:57.140 --> 22:58.140]  Let me complete first.\n",
            "[22:58.140 --> 23:01.140]  So this is the input in on the left side in 3D.\n",
            "[23:01.140 --> 23:04.140]  I'm just trying to plot the intensities as a bump map.\n",
            "[23:04.140 --> 23:09.140]  Then you see there is a edge here and there's a small high frequency details here.\n",
            "[23:09.140 --> 23:18.140]  Now if I will take the spatial uniform Gaussian filter its mask and visualize it in 3D and apply that to this.\n",
            "[23:18.140 --> 23:20.140]  What I will.\n",
            "[23:20.140 --> 23:21.140]  Sorry.\n",
            "[23:21.140 --> 23:23.140]  And I'll take the intensity weights which is the right side.\n",
            "[23:23.140 --> 23:26.140]  And if I take the multiplication this is what I will get.\n",
            "[23:26.140 --> 23:32.140]  So where the weights are close to zero I will get pretty much in this area very minimal smoothening.\n",
            "[23:32.140 --> 23:33.140]  Right.\n",
            "[23:33.140 --> 23:43.140]  While in the remaining area here where the weights where the intensity weights are what I'm putting as high that basically means it will apply regular Gaussian filtering.\n",
            "[23:43.140 --> 23:48.140]  And that net effect will be I see this flattening in this area.\n",
            "[23:48.140 --> 23:52.140]  The details are lost flattening in this area details are lost.\n",
            "[23:52.140 --> 23:54.140]  But this edge is not blurred out.\n",
            "[23:54.140 --> 23:55.140]  Right.\n",
            "[23:55.140 --> 23:57.140]  This vertical edge is kept intact.\n",
            "[23:57.140 --> 24:00.140]  This is just a 3D visualization of this patch.\n",
            "[24:00.140 --> 24:02.140]  Think of it that way.\n",
            "[24:02.140 --> 24:04.140]  So I'm blurring in this area.\n",
            "[24:04.140 --> 24:05.140]  I'm blurring in this area.\n",
            "[24:05.140 --> 24:08.140]  But I'm not blurring out the edge.\n",
            "[24:08.140 --> 24:09.140]  Right.\n",
            "[24:09.140 --> 24:10.140]  Just to understand.\n",
            "[24:10.140 --> 24:19.140]  And this is a way to really see that how does blurring impact how to really qualitatively visualize it.\n",
            "[24:19.140 --> 24:22.140]  So this is the image patch in the original image.\n",
            "[24:22.140 --> 24:28.140]  On the middle here on top on the right on top is what is after applying bilateral filtering.\n",
            "[24:28.140 --> 24:29.140]  Right.\n",
            "[24:29.140 --> 24:30.140]  Near the nose region.\n",
            "[24:30.140 --> 24:36.140]  So somewhere here I am taking somewhere in this area.\n",
            "[24:36.140 --> 24:37.140]  Right.\n",
            "[24:37.140 --> 24:45.140]  So this is a zoom in of this and this is a zoom in of the bilateral filter applied.\n",
            "[24:45.140 --> 24:46.140]  OK.\n",
            "[24:46.140 --> 24:47.140]  Now.\n",
            "[24:47.140 --> 24:51.140]  Now essentially if you see what is plotted here on the bottom.\n",
            "[24:51.140 --> 24:52.140]  OK.\n",
            "[24:52.140 --> 24:57.140]  So these are as this.\n",
            "[24:57.140 --> 25:01.140]  So on the bottom these level sets are being basically shown here.\n",
            "[25:01.140 --> 25:02.140]  So what is a level set?\n",
            "[25:02.140 --> 25:06.140]  These are contours where the intensity is same.\n",
            "[25:06.140 --> 25:07.140]  Image intensity is same.\n",
            "[25:07.140 --> 25:08.140]  Right.\n",
            "[25:08.140 --> 25:11.140]  So essentially if you take this small circle here.\n",
            "[25:11.140 --> 25:12.140]  Which is drawn here.\n",
            "[25:12.140 --> 25:17.140]  That means all pixels on this contour have the same grayscale value.\n",
            "[25:17.140 --> 25:20.140]  So these are called ISO contours of intensity.\n",
            "[25:20.140 --> 25:21.140]  Right.\n",
            "[25:21.140 --> 25:26.140]  In topographic maps you might have seen on Google or any traditional map if you see.\n",
            "[25:26.140 --> 25:33.140]  When they want to depict the height around a mountain region or around a flat area also you will see this kind of contour maps.\n",
            "[25:33.140 --> 25:34.140]  Right.\n",
            "[25:34.140 --> 25:35.140]  So these are the maps of this.\n",
            "[25:35.140 --> 25:38.140]  They are all location of the same height.\n",
            "[25:38.140 --> 25:42.140]  So if you see this contour maps pretty much disappear in this area.\n",
            "[25:42.140 --> 25:45.140]  In the middle of this of this area.\n",
            "[25:45.140 --> 25:52.140]  That because all pixels are now having same intensity it's a flat or the details are lost.\n",
            "[25:52.140 --> 25:57.140]  That means I have blurred out the detail which is also quantitatively visible here.\n",
            "[25:57.140 --> 26:02.140]  This area has this flat thing while this area has some details.\n",
            "[26:02.140 --> 26:03.140]  Right.\n",
            "[26:03.140 --> 26:09.140]  So I have blurred them out but I did not blur out the details around the nostrils if you see.\n",
            "[26:09.140 --> 26:12.140]  There I am seeing very good details.\n",
            "[26:12.140 --> 26:15.140]  They are kept intact.\n",
            "[26:15.140 --> 26:18.140]  So what is the level set?\n",
            "[26:18.140 --> 26:21.140]  Right.\n",
            "[26:21.140 --> 26:22.140]  Is that clear?\n",
            "[26:22.140 --> 26:25.140]  Any other question?\n",
            "[26:25.140 --> 26:26.140]  Hello sir.\n",
            "[26:26.140 --> 26:27.140]  I have one doubt.\n",
            "[26:27.140 --> 26:30.140]  Can you go back to the question that you asked?\n",
            "[26:30.140 --> 26:33.140]  I have one question.\n",
            "[26:33.140 --> 26:36.140]  Can you go back to the question that you asked?\n",
            "[26:36.140 --> 26:38.140]  I have one question.\n",
            "[26:38.140 --> 26:41.140]  Can you go back to the question that you asked?\n",
            "[26:41.140 --> 26:42.140]  Is that clear?\n",
            "[26:42.140 --> 26:45.140]  Any other question?\n",
            "[26:45.140 --> 26:46.140]  Hello sir.\n",
            "[26:46.140 --> 26:47.140]  I have one doubt.\n",
            "[26:47.140 --> 26:52.140]  Can you go back to the USARP filter slide please?\n",
            "[26:52.140 --> 26:55.140]  You mean unsharp masking here?\n",
            "[26:55.140 --> 26:56.140]  Yes unsharp.\n",
            "[26:56.140 --> 26:57.140]  This image.\n",
            "[26:57.140 --> 26:59.140]  Yes this one.\n",
            "[26:59.140 --> 27:05.140]  So what if I add the second and third instead of first and third.\n",
            "[27:05.140 --> 27:12.140]  So what if I add second and third image then will it get the similar result?\n",
            "[27:12.140 --> 27:14.140]  That's a very good observation.\n",
            "[27:14.140 --> 27:16.140]  It might get very close to it.\n",
            "[27:16.140 --> 27:17.140]  Yes.\n",
            "[27:17.140 --> 27:27.140]  But then if you see here in the other one the blurring was not at all in the area of the details.\n",
            "[27:27.140 --> 27:31.140]  So these edges what essentially are saying I am doing a blurring here.\n",
            "[27:31.140 --> 27:32.140]  Right.\n",
            "[27:32.140 --> 27:36.140]  These edges will be blurred out and I will again add those edge details.\n",
            "[27:36.140 --> 27:37.140]  Right.\n",
            "[27:37.140 --> 27:44.140]  That's equivalent to say that you don't do any blurring in this area.\n",
            "[27:44.140 --> 27:47.140]  You do a blurring hole in this area not around the edges.\n",
            "[27:47.140 --> 27:50.140]  But if you really see here there is one problem.\n",
            "[27:50.140 --> 27:59.140]  This area if I will apply bilateral filtering this will become flat pretty much.\n",
            "[27:59.140 --> 28:00.140]  Isn't it?\n",
            "[28:00.140 --> 28:03.140]  Because the appearance is very similar.\n",
            "[28:03.140 --> 28:04.140]  Correct.\n",
            "[28:04.140 --> 28:11.140]  But if I just do this level of blurring it won't happen that way because these edge these details will be added back.\n",
            "[28:11.140 --> 28:14.140]  Okay yes.\n",
            "[28:14.140 --> 28:15.140]  Right.\n",
            "[28:15.140 --> 28:23.140]  But again if I do some level of tuning like some way to blurring and some more way to edges then.\n",
            "[28:24.140 --> 28:25.140]  Yeah.\n",
            "[28:25.140 --> 28:28.140]  If you do thresholding of this edge map.\n",
            "[28:28.140 --> 28:29.140]  Right.\n",
            "[28:29.140 --> 28:32.140]  Maybe it is possible you will reach closer to that.\n",
            "[28:32.140 --> 28:35.140]  But then that is a more principled approach.\n",
            "[28:35.140 --> 28:36.140]  Okay.\n",
            "[28:36.140 --> 28:47.140]  Bilateral is a more principled approach where we are trying to make use of appearance disparity or distance in the difference in appearance of neighboring pixels.\n",
            "[28:48.140 --> 28:53.140]  As an important information and using that to decide whether to blur or not.\n",
            "[28:53.140 --> 28:56.140]  So that is more principled way to solve the problem.\n",
            "[28:56.140 --> 28:57.140]  Yes.\n",
            "[28:57.140 --> 29:02.140]  Thank you.\n",
            "[29:02.140 --> 29:08.140]  Go back to the image that you were showing last like that girl photo with hat.\n",
            "[29:08.140 --> 29:09.140]  Yeah.\n",
            "[29:09.140 --> 29:11.140]  Oh which one.\n",
            "[29:11.140 --> 29:12.140]  Not this one.\n",
            "[29:12.140 --> 29:14.140]  The next next.\n",
            "[29:14.140 --> 29:15.140]  Yeah this one.\n",
            "[29:15.140 --> 29:18.140]  So here if we see if we see the contours.\n",
            "[29:18.140 --> 29:22.140]  So here the these are different appearances.\n",
            "[29:22.140 --> 29:23.140]  Right.\n",
            "[29:23.140 --> 29:33.140]  The white line will be a different appearance if we see the pixel wise appearance and then how it is blurring like merging them into one.\n",
            "[29:33.140 --> 29:35.140]  That is what the blurring will do.\n",
            "[29:35.140 --> 29:36.140]  Right.\n",
            "[29:36.140 --> 29:37.140]  It will flatten the things.\n",
            "[29:37.140 --> 29:42.140]  But intensity wise if appearance wise they are different.\n",
            "[29:42.140 --> 29:43.140]  Right.\n",
            "[29:43.140 --> 29:44.140]  No no no.\n",
            "[29:44.140 --> 29:47.140]  That's what that's why we talked about this Gaussian kernel.\n",
            "[29:47.140 --> 29:48.140]  Right.\n",
            "[29:48.140 --> 29:51.140]  If the appearance is very similar.\n",
            "[29:51.140 --> 29:52.140]  Right.\n",
            "[29:52.140 --> 29:53.140]  Okay.\n",
            "[29:53.140 --> 29:57.140]  If the appearance is very similar then the weight will be high.\n",
            "[29:57.140 --> 29:59.140]  If the appearance is different.\n",
            "[29:59.140 --> 30:02.140]  So this is what is the plot of XI minus X.\n",
            "[30:02.140 --> 30:05.140]  A similar plot will exist for I of XI also.\n",
            "[30:05.140 --> 30:08.140]  Isn't it.\n",
            "[30:08.140 --> 30:09.140]  Right.\n",
            "[30:09.140 --> 30:12.140]  This is the same kernel if I use the same Gaussian kernel.\n",
            "[30:12.140 --> 30:16.140]  So if the if you really look at the X axis of this.\n",
            "[30:16.140 --> 30:17.140]  Okay.\n",
            "[30:17.140 --> 30:20.140]  Let's keep this as a capital X.\n",
            "[30:20.140 --> 30:25.140]  Now I really see here if this this is saying a small value here means what.\n",
            "[30:25.140 --> 30:35.140]  Small here value at this point if I took let's say this is point one that means that or one let's say not point one one.\n",
            "[30:35.140 --> 30:38.140]  That basically means the neighboring pixels.\n",
            "[30:38.140 --> 30:41.140]  Neighboring means the fourth neighbor or fifth neighbor.\n",
            "[30:41.140 --> 30:47.140]  Even if they have their four four three pixels apart their appearance is pretty much same.\n",
            "[30:47.140 --> 30:48.140]  Right.\n",
            "[30:48.140 --> 30:50.140]  They have a difference of only one.\n",
            "[30:50.140 --> 30:52.140]  So one is 30 one is 31.\n",
            "[30:52.140 --> 30:53.140]  Okay.\n",
            "[30:53.140 --> 30:56.140]  Which basically means this value of Y will be high.\n",
            "[30:56.140 --> 30:57.140]  Right.\n",
            "[30:57.140 --> 31:00.140]  This line after this value.\n",
            "[31:00.140 --> 31:01.140]  Right.\n",
            "[31:01.140 --> 31:02.140]  Right.\n",
            "[31:02.140 --> 31:05.140]  Not a tail distribution of e to the power minus X.\n",
            "[31:05.140 --> 31:06.140]  Right.\n",
            "[31:06.140 --> 31:07.140]  So then what will happen.\n",
            "[31:07.140 --> 31:10.140]  That means you give.\n",
            "[31:10.140 --> 31:11.140]  Good importance.\n",
            "[31:11.140 --> 31:13.140]  You do Gaussian blurring in this area.\n",
            "[31:13.140 --> 31:15.140]  That's all I'm saying.\n",
            "[31:15.140 --> 31:16.140]  Right.\n",
            "[31:16.140 --> 31:23.140]  Even if it's a three pixel apart whatever appropriate influence and averaging you get with Gaussian you do that.\n",
            "[31:23.140 --> 31:26.140]  That will make it look flatter.\n",
            "[31:26.140 --> 31:31.140]  Then any other pixel which are around the edge.\n",
            "[31:31.140 --> 31:33.140]  Those pixels will fall in this area.\n",
            "[31:33.140 --> 31:37.140]  Where the intensity difference between the two neighboring pixels is very far.\n",
            "[31:37.140 --> 31:38.140]  Right.\n",
            "[31:38.140 --> 31:43.140]  So there I will not take the I will not take the Gaussian blurring because it will be close to zero.\n",
            "[31:43.140 --> 31:47.140]  IFR output will be close to zero.\n",
            "[31:47.140 --> 31:48.140]  Right.\n",
            "[31:48.140 --> 31:51.140]  Then GES does not matter in that case.\n",
            "[31:51.140 --> 31:54.140]  So I of XI will have no importance in that case.\n",
            "[31:54.140 --> 31:55.140]  Right.\n",
            "[31:55.140 --> 31:57.140]  Even though it's a immediate neighbor.\n",
            "[31:57.140 --> 32:02.140]  Even if XI minus X is very small and hence GES output is large.\n",
            "[32:02.140 --> 32:04.140]  But FR output is zero.\n",
            "[32:04.140 --> 32:07.140]  That means the whole thing will turn out to be zero.\n",
            "[32:07.140 --> 32:12.140]  So that means I of XI will have no influence on that blurring of that pixel.\n",
            "[32:12.140 --> 32:13.140]  Right.\n",
            "[32:13.140 --> 32:22.140]  This literally means this pixel here will have no influence of this pixel.\n",
            "[32:22.140 --> 32:23.140]  Right.\n",
            "[32:23.140 --> 32:27.140]  Because other side of edge will not help.\n",
            "[32:27.140 --> 32:28.140]  If I do that.\n",
            "[32:28.140 --> 32:33.140]  If I do averaging along all this that basically means I'm blurring out edge.\n",
            "[32:33.140 --> 32:34.140]  Right.\n",
            "[32:34.140 --> 32:43.140]  So pixels near the edge they will not get blurring because of the appearance of pixels in the other side of the neighborhood.\n",
            "[32:43.140 --> 32:48.140]  I don't know if I'm making any sense.\n",
            "[32:48.140 --> 32:53.140]  So what I'm going to say if I take these three pixels let me redraw it again.\n",
            "[32:53.140 --> 32:56.140]  A pixel here is the center pixel of the window.\n",
            "[32:56.140 --> 32:57.140]  Right.\n",
            "[32:57.140 --> 32:58.140]  Okay.\n",
            "[32:58.140 --> 33:06.140]  When I am doing a bilateral filtering I will get due influence of all these pixels.\n",
            "[33:06.140 --> 33:11.140]  So the appearance of this pixel will be an averaging of all these pixels.\n",
            "[33:11.140 --> 33:15.140]  With appropriate motion weights.\n",
            "[33:15.140 --> 33:18.140]  But these pixels on the right side will not contribute.\n",
            "[33:18.140 --> 33:25.140]  Is that is it making sense?\n",
            "[33:25.140 --> 33:26.140]  Yes.\n",
            "[33:26.140 --> 33:27.140]  Yes.\n",
            "[33:27.140 --> 33:28.140]  Yes.\n",
            "[33:28.140 --> 33:29.140]  Yeah.\n",
            "[33:29.140 --> 33:30.140]  These pixels will not contribute.\n",
            "[33:30.140 --> 33:31.140]  Okay.\n",
            "[33:31.140 --> 33:32.140]  We have to move on.\n",
            "[33:32.140 --> 33:33.140]  We're going very slow.\n",
            "[33:33.140 --> 33:43.140]  So this is one example of like what you call beautification so called can be achieved.\n",
            "[33:43.140 --> 33:44.140]  Right.\n",
            "[33:44.140 --> 33:50.140]  So you can see that cheeks area they got blurred out significantly forehead area is blurred out.\n",
            "[33:50.140 --> 33:52.140]  But eyes they retain their details.\n",
            "[33:52.140 --> 33:53.140]  Right.\n",
            "[33:53.140 --> 33:54.140]  The lips retain their details.\n",
            "[33:54.140 --> 33:55.140]  Okay.\n",
            "[33:55.140 --> 33:56.140]  Is that clear?\n",
            "[33:56.140 --> 33:57.140]  The hairlines retain their details.\n",
            "[33:57.140 --> 33:58.140]  We can see change of color also in the two images.\n",
            "[33:58.140 --> 33:59.140]  What is it?\n",
            "[33:59.140 --> 34:00.140]  So the lips color has magnified hair color.\n",
            "[34:00.140 --> 34:01.140]  We could see a bit of.\n",
            "[34:01.140 --> 34:02.140]  It's an averaging that is happening.\n",
            "[34:02.140 --> 34:03.140]  You see the details of the lips are lost.\n",
            "[34:03.140 --> 34:04.140]  The fine lines on the lips.\n",
            "[34:04.140 --> 34:05.140]  They are blurred out.\n",
            "[34:05.140 --> 34:28.140]  This is a color change is what your mind is perceiving because now it is seeing much\n",
            "[34:28.140 --> 34:29.140]  more homogeneous image.\n",
            "[34:29.140 --> 34:30.140]  Okay.\n",
            "[34:30.140 --> 34:31.140]  Right.\n",
            "[34:32.140 --> 34:37.140]  We are not adding new colors.\n",
            "[34:37.140 --> 34:39.140]  You can try it yourself.\n",
            "[34:39.140 --> 34:43.140]  Just take some code or write your own function and then try it.\n",
            "[34:43.140 --> 34:45.140]  Now another question that one might ask.\n",
            "[34:45.140 --> 34:47.140]  We looked at image intensities.\n",
            "[34:47.140 --> 34:49.140]  You're now talking about color.\n",
            "[34:49.140 --> 34:50.140]  Right.\n",
            "[34:50.140 --> 34:52.140]  So color is three channels RGB.\n",
            "[34:52.140 --> 34:58.140]  So again, the same can be extended that you can compute the kernel FR and GR instead of\n",
            "[34:58.140 --> 34:59.140]  real values.\n",
            "[34:59.140 --> 35:00.140]  Intensity values.\n",
            "[35:00.140 --> 35:06.140]  You can use a 3D vector of RGB color and compute the similarity because all we are using this\n",
            "[35:06.140 --> 35:07.140]  function is a vector.\n",
            "[35:07.140 --> 35:08.140]  Right.\n",
            "[35:08.140 --> 35:11.140]  So this is an L2 norm if you see.\n",
            "[35:11.140 --> 35:12.140]  Right.\n",
            "[35:12.140 --> 35:15.140]  So this L2 norm is valid for vectors also.\n",
            "[35:15.140 --> 35:19.140]  So instead of I of XI, this intensity is a single value, real value.\n",
            "[35:19.140 --> 35:22.140]  I can have a 3D vector here corresponding to RGB.\n",
            "[35:22.140 --> 35:24.140]  Similarly in RGB here.\n",
            "[35:24.140 --> 35:27.140]  Same applies to Gaussian blurring also.\n",
            "[35:27.140 --> 35:28.140]  Okay.\n",
            "[35:30.140 --> 35:31.140]  Okay.\n",
            "[35:31.140 --> 35:34.140]  So this is another example of noisy input.\n",
            "[35:34.140 --> 35:38.140]  Also, if you see here, because in the evening if you take you sometime get this kind of\n",
            "[35:38.140 --> 35:39.140]  image.\n",
            "[35:39.140 --> 35:43.140]  So these areas are blurred out, but these edges are retained very clearly.\n",
            "[35:43.140 --> 35:47.140]  Edges of the dome and these kind of things.\n",
            "[35:47.140 --> 35:48.140]  Okay.\n",
            "[35:50.140 --> 35:53.140]  Sir, what is the exact function FR?\n",
            "[35:53.140 --> 35:57.140]  FR could be one example of that is Gaussian.\n",
            "[35:57.140 --> 35:59.140]  You can use Gaussian which is plotted here.\n",
            "[35:59.140 --> 36:01.140]  This is FR.\n",
            "[36:01.140 --> 36:05.140]  The one which is shown here on the grays.\n",
            "[36:05.140 --> 36:06.140]  Right.\n",
            "[36:06.140 --> 36:09.140]  That is a Gaussian kernel.\n",
            "[36:09.140 --> 36:11.140]  Inverse Gaussian.\n",
            "[36:11.140 --> 36:14.140]  It is Gaussian, not inverse Gaussian.\n",
            "[36:14.140 --> 36:18.140]  It is V raise power minus X is the Gaussian.\n",
            "[36:18.140 --> 36:20.140]  V raise power X is exponential.\n",
            "[36:20.140 --> 36:24.140]  So Gaussian is the inverse exponential family of function.\n",
            "[36:24.140 --> 36:25.140]  Yes.\n",
            "[36:25.140 --> 36:26.140]  Okay.\n",
            "[36:26.140 --> 36:27.140]  This is all for depiction.\n",
            "[36:27.140 --> 36:31.140]  You have to be very careful because Gaussian is just not E raise power minus X.\n",
            "[36:31.140 --> 36:33.140]  You also do a division by variance.\n",
            "[36:33.140 --> 36:35.140]  Right.\n",
            "[36:35.140 --> 36:39.140]  If you want, I can write that because otherwise you might.\n",
            "[36:39.140 --> 36:42.140]  That's why video is more important than slide.\n",
            "[36:42.140 --> 36:47.140]  If you really do that, I can just put below that some sigma also.\n",
            "[36:47.140 --> 36:48.140]  Right.\n",
            "[36:50.140 --> 36:51.140]  Okay.\n",
            "[36:51.140 --> 36:56.140]  Then it becomes an exponential Gaussian kind of family.\n",
            "[36:56.140 --> 36:57.140]  Okay.\n",
            "[36:57.140 --> 37:03.140]  So we'll stop here for this part and then we'll start the new part, which is more on\n",
            "[37:03.140 --> 37:04.140]  image transformation.\n",
            "[37:04.140 --> 37:05.140]  Okay.\n",
            "[37:05.140 --> 37:07.140]  Let me start with that.\n",
            "[37:12.140 --> 37:15.140]  So we'll talk about geometrical transformation on the images.\n",
            "[37:15.140 --> 37:20.140]  And this has importance in terms of when we want to do image warping.\n",
            "[37:20.140 --> 37:21.140]  Okay.\n",
            "[37:21.140 --> 37:28.140]  So these are some examples of when I want to transform an input image of Apple, which is there in the middle.\n",
            "[37:28.140 --> 37:33.140]  If I do a rotation of this Apple image, what I want to see is this image.\n",
            "[37:33.140 --> 37:35.140]  Right.\n",
            "[37:35.140 --> 37:42.140]  If I do a scaling, I want to see this, which basically means you compress the image.\n",
            "[37:42.140 --> 37:44.140]  Y axis is compressed.\n",
            "[37:44.140 --> 37:46.140]  X axis is elongated.\n",
            "[37:46.140 --> 37:49.140]  Then you do a shearing, which is shown here.\n",
            "[37:49.140 --> 37:53.140]  You can also do translation, which I'm not showing here.\n",
            "[37:53.140 --> 37:55.140]  Then there are two more transformation.\n",
            "[37:55.140 --> 38:00.140]  In fact, three more that you will see today or in this lecture today and maybe next class if needed.\n",
            "[38:00.140 --> 38:07.140]  Affine transformation, projective transformation and polynomial transformation.\n",
            "[38:07.140 --> 38:08.140]  Right.\n",
            "[38:08.140 --> 38:11.140]  So how to really get this kind of effect in the image?\n",
            "[38:11.140 --> 38:13.140]  So these all falls in the purview of computer vision.\n",
            "[38:13.140 --> 38:18.140]  In fact, this affine and projective transformations are very important for the second sector also,\n",
            "[38:18.140 --> 38:22.140]  where we will learn about the camera modeling and perspective imaging and all.\n",
            "[38:22.140 --> 38:25.140]  So we'll just basics we will introduce now.\n",
            "[38:25.140 --> 38:28.140]  Foundation will build here in this sector.\n",
            "[38:28.140 --> 38:32.140]  So these are the polynomial kernels are what is shown on the bottom.\n",
            "[38:32.140 --> 38:35.140]  So if you really see, this is A is the original image.\n",
            "[38:35.140 --> 38:37.140]  B is the translated image.\n",
            "[38:37.140 --> 38:39.140]  C is the scaled image.\n",
            "[38:39.140 --> 38:41.140]  Fourth is B is the rotated image.\n",
            "[38:41.140 --> 38:45.140]  E is the, if I'm correct, affine transformation.\n",
            "[38:45.140 --> 38:52.140]  And F is the polynomial transformation, which is a twirl effect.\n",
            "[38:52.140 --> 38:53.140]  Right.\n",
            "[38:53.140 --> 39:03.140]  So today in this session, I mean in next session, we'll see how to mathematically understand and induce this transformation in the images.\n",
            "[39:03.140 --> 39:05.140]  So what are the applications?\n",
            "[39:05.140 --> 39:06.140]  There are multiple applications.\n",
            "[39:06.140 --> 39:10.140]  If I can correct the images for camera orientation.\n",
            "[39:10.140 --> 39:16.140]  So if you do a stitching of the images, then you take the images you can show on the right top.\n",
            "[39:16.140 --> 39:19.140]  Multiple images of the same scene is taken.\n",
            "[39:19.140 --> 39:22.140]  Different parts are taken with some overlap.\n",
            "[39:22.140 --> 39:25.140]  But the orientation of camera is very different.\n",
            "[39:25.140 --> 39:30.140]  Somewhere it's portrait, somewhere it's landscape and it's rotated even if it is portrait.\n",
            "[39:30.140 --> 39:34.140]  So you would like to align these images and then stitch them together.\n",
            "[39:34.140 --> 39:42.140]  So aligning the image for stitching, correcting for the camera orientation, all that needs to transform the image.\n",
            "[39:42.140 --> 39:44.140]  Then correcting for lens distortion.\n",
            "[39:44.140 --> 39:50.140]  So what the image in the middle what you're showing, these two images, the left image is what you get from camera.\n",
            "[39:50.140 --> 39:58.140]  This is because from a particular lens distortion that happens when you have a lens of high power, that's a barrel distortion.\n",
            "[39:58.140 --> 40:02.140]  So you can correct for that and get this image by applying.\n",
            "[40:02.140 --> 40:07.140]  Of course, you need to estimate the parameters of this distortion, but the mathematical modeling of that.\n",
            "[40:07.140 --> 40:14.140]  And assuming if I know the parameters, how do I from this image generate this image is what we will study.\n",
            "[40:14.140 --> 40:17.140]  Then it can also be very useful in image morphing.\n",
            "[40:17.140 --> 40:22.140]  When I want to take two images and then I want to find a gradual transformation between the two.\n",
            "[40:22.140 --> 40:26.140]  So the kind of deformations you are seeing in this GIF image is what we are interested.\n",
            "[40:26.140 --> 40:32.140]  And this kind of interesting effects also that we will touch upon.\n",
            "[40:32.140 --> 40:37.140]  So let's try to mathematically model this transformation.\n",
            "[40:37.140 --> 40:49.140]  So essentially what we are doing is we are applying some geometric operations to transform the image I to new image I' by modifying coordinates of the image pixel.\n",
            "[40:49.140 --> 40:58.140]  So particular pixel in the input image x, y is now transformed to a new pixel x' y'.\n",
            "[40:58.140 --> 41:04.140]  Or in a way I am getting an I' by applying this transformation on x and y.\n",
            "[41:04.140 --> 41:13.140]  So when I do a translation, it is simply a motion. Motion means along in the xy plane.\n",
            "[41:13.140 --> 41:22.140]  So I'm simply adding component of dx and dy to x and y, which basically means I'm shifting the location of the image pixel.\n",
            "[41:22.140 --> 41:27.140]  So this is easiest to understand.\n",
            "[41:27.140 --> 41:38.140]  So all I'm doing is I'm adding dx to x component, x coordinate, I'm adding dy to y coordinate and I'm just rendering the image.\n",
            "[41:38.140 --> 41:43.140]  So now the x' becomes x plus dx, y' becomes y plus dy.\n",
            "[41:43.140 --> 41:48.140]  And dx and dy are the coefficient of translation vector.\n",
            "[41:48.140 --> 41:53.140]  How much I want to translate along x and y. Is this clear? Easy?\n",
            "[41:53.140 --> 41:59.140]  Overall image which we are denoting by I, it will be same.\n",
            "[41:59.140 --> 42:02.140]  Why should it become I'?\n",
            "[42:02.140 --> 42:09.140]  This is just a transformation because the image might get transformed in the other operations.\n",
            "[42:09.140 --> 42:16.140]  Translation is easy. So image details remain as it is, just their location is changing here.\n",
            "[42:16.140 --> 42:24.140]  So it's a translation. For translation Ixy can be Ix dash y dash. Right sir?\n",
            "[42:24.140 --> 42:30.140]  No, Ix dash y dash will have different intensity value. Right?\n",
            "[42:30.140 --> 42:36.140]  After transformation, sir, it is getting changed.\n",
            "[42:36.140 --> 42:40.140]  I'm talking about just translation where position is getting changed.\n",
            "[42:40.140 --> 42:46.140]  Let me tell you, what is your problem. Your dear asking is why I'm depicting this new image as I dash.\n",
            "[42:46.140 --> 42:49.140]  Right sir.\n",
            "[42:49.140 --> 42:54.140]  Let me explain. Let me answer that.\n",
            "[42:54.140 --> 42:59.140]  So this is, when I say an image, if you see the frame is also an image.\n",
            "[42:59.140 --> 43:05.140]  The image boundary is this. The gray area is also part of the image.\n",
            "[43:05.140 --> 43:09.140]  So if this image and this image is same.\n",
            "[43:09.140 --> 43:12.140]  If we consider frame part of that, they are not same.\n",
            "[43:12.140 --> 43:17.140]  It's not even a frame. It's a background that I'm modeling an extra buffer region I'm giving.\n",
            "[43:17.140 --> 43:24.140]  What I'm trying to tell you, I of x, y is let's say the corner pixel here.\n",
            "[43:24.140 --> 43:30.140]  Is not same as I of x plus dx and y plus dx.\n",
            "[43:30.140 --> 43:34.140]  Because I of x plus ds and y plus ds will be somewhere here.\n",
            "[43:34.140 --> 43:38.140]  So it is some other I dash that I'm saying.\n",
            "[43:38.140 --> 43:42.140]  So if you really look at coding wise, if you really have to represent this in computer,\n",
            "[43:42.140 --> 43:48.140]  you will have this image represented in some frame buffer in some 2D array.\n",
            "[43:48.140 --> 43:54.140]  Now I have to create a new array in which I have to copy these values with appropriate shifting of x and y.\n",
            "[43:54.140 --> 43:58.140]  So it makes sense to represent them as I and I dash.\n",
            "[43:58.140 --> 44:01.140]  I dash is a transformed image.\n",
            "[44:01.140 --> 44:04.140]  Got it. Thank you.\n",
            "[44:04.140 --> 44:07.140]  Someone else was making some question or something else.\n",
            "[44:07.140 --> 44:12.140]  No, no, no, sir. I was actually telling the same because that only I.\n",
            "[44:12.140 --> 44:16.140]  Yeah, let me do that. If I fail in my job, then you can help.\n",
            "[44:16.140 --> 44:19.140]  No, no, no, sir. I was telling from the different.\n",
            "[44:19.140 --> 44:21.140]  Problem is in online class.\n",
            "[44:21.140 --> 44:25.140]  So then, yes, in preference.\n",
            "[44:25.140 --> 44:28.140]  So one question on this.\n",
            "[44:28.140 --> 44:33.140]  I can see in this image, the whole image is shifted to a right side.\n",
            "[44:33.140 --> 44:37.140]  Then there will be few pixels who are new.\n",
            "[44:37.140 --> 44:40.140]  So it should become white.\n",
            "[44:40.140 --> 44:43.140]  They don't have any new pixels remaining to it.\n",
            "[44:43.140 --> 44:49.140]  No, no. So that is where the background gray area is shown here just to alleviate that concern.\n",
            "[44:49.140 --> 44:55.140]  Then the the left part remaining, we make it a gray or some.\n",
            "[44:55.140 --> 44:58.140]  That is here, right?\n",
            "[44:58.140 --> 45:01.140]  So I'm assuming there is a canvas.\n",
            "[45:01.140 --> 45:04.140]  On which I am shifting this image.\n",
            "[45:04.140 --> 45:12.140]  Background canvas has some appearance in this example, gray that remain gray after shifting.\n",
            "[45:12.140 --> 45:20.140]  That means I am overwriting those pixels with background again, because I'm moving the X comma Y pixel to a new location.\n",
            "[45:20.140 --> 45:24.140]  So all I'm doing is you can say cut, copy and paste.\n",
            "[45:24.140 --> 45:30.140]  I'm cutting it, the appearance of X comma Y and pasting it X plus DX location.\n",
            "[45:30.140 --> 45:34.140]  But I am not operating on the same image.\n",
            "[45:34.140 --> 45:39.140]  OK, I am taking creating a new blank canvas on which I am pasting it.\n",
            "[45:39.140 --> 45:43.140]  This is very important to understand. That's why I use the example of array.\n",
            "[45:43.140 --> 45:46.140]  I'm creating a new array. This is not happening in the same array.\n",
            "[45:46.140 --> 45:51.140]  Otherwise, by the time I will reach here, I will get inconsistent appearance.\n",
            "[45:51.140 --> 45:59.140]  Right. So I create a new canvas on which I'm pasting the appearance from the current image I to I dash.\n",
            "[45:59.140 --> 46:01.140]  Clear?\n",
            "[46:01.140 --> 46:03.140]  Sir, please correct me, sir.\n",
            "[46:03.140 --> 46:12.140]  So this kind of example is helpful to get the alignment which we have seen the previous slides, top right one.\n",
            "[46:12.140 --> 46:14.140]  Is that correct in understanding?\n",
            "[46:14.140 --> 46:16.140]  Yes, this is one of the operation.\n",
            "[46:16.140 --> 46:18.140]  There are many more operations we will need.\n",
            "[46:18.140 --> 46:21.140]  This is one of the operations.\n",
            "[46:21.140 --> 46:25.140]  And we don't lose the picture quality by doing this.\n",
            "[46:25.140 --> 46:27.140]  We will talk about it.\n",
            "[46:27.140 --> 46:32.140]  Translation is no major issue because translation is along X and Y axis.\n",
            "[46:32.140 --> 46:35.140]  So our pixel resolution is not being affected.\n",
            "[46:35.140 --> 46:38.140]  Quality is not being affected.\n",
            "[46:38.140 --> 46:40.140]  But it's a good question.\n",
            "[46:40.140 --> 46:42.140]  OK.\n",
            "[46:42.140 --> 46:45.140]  Shall I move on? Translation is easy? Clear?\n",
            "[46:45.140 --> 46:47.140]  Yes, sir.\n",
            "[46:47.140 --> 46:49.140]  Questions are clear? OK.\n",
            "[46:49.140 --> 46:52.140]  Now, all I am doing is shift by vector.\n",
            "[46:52.140 --> 46:54.140]  Let's write it mathematically.\n",
            "[46:54.140 --> 46:56.140]  So I'm doing a vector vector addition.\n",
            "[46:56.140 --> 47:00.140]  XY is the 2D vector representing the coordinate of the pixel.\n",
            "[47:00.140 --> 47:04.140]  And I'm simply doing a DX, DY is another 2D vector.\n",
            "[47:04.140 --> 47:05.140]  I'm adding to it.\n",
            "[47:05.140 --> 47:10.140]  Right. And then I'm copying the appearances from XY to X dash Y dash.\n",
            "[47:10.140 --> 47:12.140]  OK.\n",
            "[47:12.140 --> 47:20.140]  Now, if I do a scaling, which basically means I want to stretch the image along Y axis in this example\n",
            "[47:20.140 --> 47:28.140]  and compress it along X axis by a factor of, let's say, 2 or 1, whatever, 1.1, 1.2, whatever it is,\n",
            "[47:28.140 --> 47:35.140]  by some SX and XY, that is equivalent of multiplying X with SX to get X dash,\n",
            "[47:35.140 --> 47:38.140]  multiplying Y with SY to get Y dash.\n",
            "[47:38.140 --> 47:45.140]  And I can write it as a matrix vector multiplication, where this matrix is the diagonal matrix,\n",
            "[47:45.140 --> 47:50.140]  which has SX, XY as the non-zero element and XY is the input vector.\n",
            "[47:50.140 --> 47:53.140]  If I multiply this, I will get X dash and Y dash.\n",
            "[47:53.140 --> 47:55.140]  What is shown up there?\n",
            "[47:55.140 --> 47:57.140]  Is this clear?\n",
            "[47:57.140 --> 48:01.140]  Mathematical depiction of this geometrical transformation of coordinates.\n",
            "[48:01.140 --> 48:03.140]  That's all we are writing here.\n",
            "[48:06.140 --> 48:19.140]  So basically, for every X dash, Y dash location in the output image, I dash, I want to know, right,\n",
            "[48:19.140 --> 48:23.140]  which XY I have to map eventually.\n",
            "[48:23.140 --> 48:29.140]  So what is shown here is, for a given XY, where to map it, isn't it?\n",
            "[48:29.140 --> 48:33.140]  What I know in input image, I know the pixel location XY.\n",
            "[48:33.140 --> 48:37.140]  I, given XY, I know X dash, Y dash.\n",
            "[48:37.140 --> 48:41.140]  If I know SX and XY, SX and SY, is that correct?\n",
            "[48:43.140 --> 48:46.140]  So I can take, let me draw that.\n",
            "[48:46.140 --> 49:07.140]  For every, any pixel I, J in this image, I will know its location in this image, right?\n",
            "[49:07.140 --> 49:10.140]  This is the forward mapping.\n",
            "[49:10.140 --> 49:20.140]  Okay, given X and Y, this is given to me, I will, I will compute this.\n",
            "[49:20.140 --> 49:23.140]  Is that correct?\n",
            "[49:23.140 --> 49:27.140]  And then I will simply copy the appearance of this pixel to this pixel.\n",
            "[49:27.140 --> 49:29.140]  Am I making sense?\n",
            "[49:29.140 --> 49:31.140]  Yes, sir.\n",
            "[49:31.140 --> 49:33.140]  Okay.\n",
            "[49:33.140 --> 49:35.140]  So this is the scaling transform.\n",
            "[49:36.140 --> 49:39.140]  Now we can put another matrix instead of this.\n",
            "[49:39.140 --> 49:41.140]  Sir, I have one doubt.\n",
            "[49:41.140 --> 49:43.140]  Can you go to the previous slide here?\n",
            "[49:43.140 --> 49:47.140]  So how the first equation it is coming, means how it is forming?\n",
            "[49:47.140 --> 49:50.140]  You look at it yourself.\n",
            "[49:50.140 --> 49:57.140]  If I have to stretch it along the axis, I will take SY more than one.\n",
            "[49:57.140 --> 50:00.140]  I can take 1.1, 1.2.\n",
            "[50:00.140 --> 50:02.140]  So Y dot will stretch.\n",
            "[50:02.140 --> 50:07.140]  X coordinate will be less than one if I want to shrink it.\n",
            "[50:07.140 --> 50:08.140]  Okay.\n",
            "[50:08.140 --> 50:10.140]  Yeah, understood.\n",
            "[50:10.140 --> 50:14.140]  But when I am modifying X, I am not modifying Y simultaneously.\n",
            "[50:14.140 --> 50:16.140]  First equation is only modifying X.\n",
            "[50:16.140 --> 50:20.140]  Second equation is only modifying Y on the top.\n",
            "[50:20.140 --> 50:22.140]  I am just writing it as a matrix.\n",
            "[50:22.140 --> 50:25.140]  So this is a system of linear equations.\n",
            "[50:25.140 --> 50:26.140]  Okay.\n",
            "[50:26.140 --> 50:29.140]  SX and SY are the linear coefficients.\n",
            "[50:29.140 --> 50:32.140]  X and Y is being modified to get X dash and Y dash.\n",
            "[50:32.140 --> 50:36.140]  If you have studied that long back in your high school, possibly.\n",
            "[50:36.140 --> 50:38.140]  Right?\n",
            "[50:38.140 --> 50:40.140]  Yeah.\n",
            "[50:40.140 --> 50:42.140]  In fact, I can...\n",
            "[50:42.140 --> 50:47.140]  Sir, scaling can be only in X and only in Y also or in all the direction also.\n",
            "[50:47.140 --> 50:49.140]  So whatever happens is scaling, right?\n",
            "[50:49.140 --> 50:51.140]  That is what the point is.\n",
            "[50:51.140 --> 50:52.140]  Yeah.\n",
            "[50:52.140 --> 50:53.140]  That is what the point is.\n",
            "[50:53.140 --> 50:56.140]  How many coordinate axes you have in 2D?\n",
            "[50:56.140 --> 50:57.140]  X and Y.\n",
            "[50:57.140 --> 50:59.140]  Yeah, right, sir. Right, right.\n",
            "[50:59.140 --> 51:00.140]  Right?\n",
            "[51:00.140 --> 51:01.140]  Yeah.\n",
            "[51:01.140 --> 51:04.140]  If you rotate it or shear, I will talk about that later in the next slide.\n",
            "[51:04.140 --> 51:10.140]  Then you will see that you can also stretch along arbitrary direction by linear combination with two axes.\n",
            "[51:10.140 --> 51:13.140]  Ultimately, you always operate on two axes in 2D.\n",
            "[51:13.140 --> 51:14.140]  Right?\n",
            "[51:14.140 --> 51:17.140]  But you can operate jointly on that.\n",
            "[51:17.140 --> 51:19.140]  That is called shear.\n",
            "[51:19.140 --> 51:26.140]  So that means while I am in X, I am adding some component of Y also.\n",
            "[51:26.140 --> 51:30.140]  So that becomes X plus BX dot Y.\n",
            "[51:30.140 --> 51:31.140]  Right?\n",
            "[51:31.140 --> 51:33.140]  So this is an example of that on the right side.\n",
            "[51:33.140 --> 51:37.140]  I am adding 0.2% of Y into X.\n",
            "[51:37.140 --> 51:41.140]  That means I am making a square as parallelogram.\n",
            "[51:41.140 --> 51:42.140]  Right?\n",
            "[51:42.140 --> 51:43.140]  Why?\n",
            "[51:43.140 --> 51:45.140]  Because Y axis is not affected here.\n",
            "[51:45.140 --> 51:48.140]  For every Y here, the same Y is maintained.\n",
            "[51:48.140 --> 51:55.140]  Only the X is now increased by a little bit percentage of Y.\n",
            "[51:55.140 --> 52:01.140]  So that's like stretching in one direction, which is not parallel to X or Y.\n",
            "[52:01.140 --> 52:03.140]  Right? Shearing.\n",
            "[52:03.140 --> 52:09.140]  I can also do the same on Y also that it will be literally become stretching in diagonally.\n",
            "[52:09.140 --> 52:10.140]  Right?\n",
            "[52:10.140 --> 52:17.140]  So here, if I really see this for this particular thing, this matrix will be 1, 0.2\n",
            "[52:17.140 --> 52:21.140]  and the bottom will be 0, 1.\n",
            "[52:21.140 --> 52:28.140]  So Y dash will remain as Y, but X will become X plus 0.2.\n",
            "[52:28.140 --> 52:32.140]  OK?\n",
            "[52:32.140 --> 52:38.140]  But this BY need not to be 0 always.\n",
            "[52:38.140 --> 52:40.140]  Is this making sense?\n",
            "[52:40.140 --> 52:42.140]  What we are doing?\n",
            "[52:42.140 --> 52:46.140]  If I want to deform, this grid is representing the image pixels.\n",
            "[52:46.140 --> 52:57.140]  So I can deform the image slightly called shear transformation by adding a component of X into Y and component of Y into X\n",
            "[52:57.140 --> 53:07.140]  to get this kind of deformation called shearing transform.\n",
            "[53:07.140 --> 53:11.140]  So in this case, BX and BY should not be 0.\n",
            "[53:11.140 --> 53:12.140]  Right?\n",
            "[53:12.140 --> 53:16.140]  In that case, in this case, what is shown on the right side?\n",
            "[53:16.140 --> 53:19.140]  BX will be 0.2, BY will be 0.\n",
            "[53:19.140 --> 53:21.140]  Right.\n",
            "[53:21.140 --> 53:22.140]  Correct.\n",
            "[53:22.140 --> 53:33.140]  But one possibility that I'm talking if I really want to stretch it like this, which basically means it should become like this.\n",
            "[53:33.140 --> 53:34.140]  Right?\n",
            "[53:34.140 --> 53:41.140]  In that case, BX will be non-zero, BY also will be non-zero because Y is also scaled, not just X.\n",
            "[53:41.140 --> 53:54.140]  So, shearing is, in this example particularly, we are scaling the image diagonally.\n",
            "[53:54.140 --> 53:57.140]  Sorry, we are scaling the image?\n",
            "[53:57.140 --> 53:59.140]  Diagonally.\n",
            "[53:59.140 --> 54:05.140]  Diagonally in which example you are saying, the red one that I have drawn now or the black one that is already drawn?\n",
            "[54:05.140 --> 54:08.140]  The right one which we have drawn.\n",
            "[54:08.140 --> 54:12.140]  In that case, it is being stretched both in X and Y, right?\n",
            "[54:12.140 --> 54:19.140]  Yes, sir. Both X and Y transformations are happening.\n",
            "[54:19.140 --> 54:25.140]  So, can we look at this one as part of scaling as well, sir?\n",
            "[54:25.140 --> 54:29.140]  It's called shearing.\n",
            "[54:29.140 --> 54:33.140]  Okay, so there is a clear difference between scaling and shearing.\n",
            "[54:33.140 --> 54:35.140]  Yes.\n",
            "[54:35.140 --> 54:43.140]  In scaling, we are adding some component. In shearing, we are also adding Y component into X. So, that becomes different.\n",
            "[54:43.140 --> 54:49.140]  No, no, no. In scaling, this off diagonal elements are zero.\n",
            "[54:49.140 --> 54:52.140]  Yes, the scaling is just factoring, right?\n",
            "[54:52.140 --> 55:00.140]  We are scaling each axis independently. Here we are taking a linear combination of both axis.\n",
            "[55:00.140 --> 55:05.140]  X plus 0.2 of Y is a linear equation, right?\n",
            "[55:05.140 --> 55:11.140]  I am taking component of Y adding to X, a linear component of Y.\n",
            "[55:11.140 --> 55:14.140]  I am not putting e raised to the power of Y.\n",
            "[55:14.140 --> 55:18.140]  Okay, so hence it is a linear equation.\n",
            "[55:18.140 --> 55:26.140]  Okay, clear?\n",
            "[55:26.140 --> 55:34.140]  The next transformation is called rotation, right?\n",
            "[55:34.140 --> 55:41.140]  Now, ideally, I derive this in the class, okay?\n",
            "[55:41.140 --> 55:47.140]  You have to use some basic trigonometry formulas and tricks.\n",
            "[55:47.140 --> 55:54.140]  So, essentially, think of it. This is my original vector, X, Y, a point basically.\n",
            "[55:54.140 --> 55:58.140]  This is the origin, okay? So, this is 0, 0.\n",
            "[55:58.140 --> 56:03.140]  So, every point in 2D can be drawn as a vector line, right?\n",
            "[56:03.140 --> 56:10.140]  Arrow here. Now, if I have to rotate this, I have to rotate by how much theta angle\n",
            "[56:10.140 --> 56:16.140]  and the convention is the negative, the counterclockwise is positive theta value.\n",
            "[56:16.140 --> 56:24.140]  So, this will be the new location of this 2D point after rotation by theta along the origin.\n",
            "[56:24.140 --> 56:26.140]  Rotation always happens around the origin.\n",
            "[56:26.140 --> 56:31.140]  So, now this becomes a new coordinate of this point will become X dash Y dash.\n",
            "[56:31.140 --> 56:43.140]  And I can write this X dash Y dash mathematically as X dash becomes X times X into cosine of theta minus minus y of sine of theta, right?\n",
            "[56:43.140 --> 56:46.140]  Similarly, Y dash is X sine theta plus Y cos theta.\n",
            "[56:46.140 --> 56:53.140]  How we come to that? I'll tell you, okay?\n",
            "[56:53.140 --> 57:00.140]  So, now I can take this triangle. There are two triangles here shown here, right?\n",
            "[57:00.140 --> 57:04.140]  Let's call this as O. This whole thing is called, let's call it as O.\n",
            "[57:04.140 --> 57:09.140]  I can also choose the color of this thing, right?\n",
            "[57:09.140 --> 57:21.140]  So, let's call this point as O. Let's call this point as A.\n",
            "[57:21.140 --> 57:25.140]  And now I'm giving this point as name B, okay? Don't get confused.\n",
            "[57:25.140 --> 57:32.140]  This is the same point after rotation. And I'm just calling this point as C and this point as B.\n",
            "[57:32.140 --> 57:39.140]  This is a projection of A and B on x-axis. This is an x-axis and y-axis.\n",
            "[57:39.140 --> 57:47.140]  Now, okay, and these are coordinates. Don't get confused with this. This is capital X. Think of that, okay?\n",
            "[57:47.140 --> 57:56.140]  Now, essentially, there is a triangle which is OAC, right?\n",
            "[57:56.140 --> 58:02.140]  It's a right angle triangle. This is 90 degree. This is also 90 degree, okay?\n",
            "[58:02.140 --> 58:11.140]  Now, for OAC, I have certain relationships, right?\n",
            "[58:11.140 --> 58:23.140]  Yes or no? I have some, what is the angle it is substituting? The obtuse angle is alpha, right?\n",
            "[58:23.140 --> 58:32.140]  Yes. Okay. Now, I can write this vector x, y, right?\n",
            "[58:32.140 --> 58:40.140]  This vector x, y, let's say it has a length h, okay?\n",
            "[58:40.140 --> 58:46.140]  So, magnitude of O is h, which is this length. From O to A, the length is h.\n",
            "[58:46.140 --> 58:56.140]  The same length also OB has. OB is also same vector, just rotated. So, its length will also be h, yes or no?\n",
            "[58:56.140 --> 59:12.140]  Yes, sir. Right. Now, this h we can write in relation to the triangle OAC, right?\n",
            "[59:12.140 --> 59:20.140]  And typically, I write triangle like this, triangle OAC. In that relation to that, I can represent h as what?\n",
            "[59:20.140 --> 59:30.140]  In terms of sine and cosine. What is x? X is this distance from O to C, right?\n",
            "[59:30.140 --> 59:36.140]  So, h cos alpha is equal to x.\n",
            "[59:36.140 --> 59:49.140]  X is h cos alpha.\n",
            "[59:49.140 --> 59:55.140]  Y is sine alpha, h sine alpha.\n",
            "[59:56.140 --> 01:00:03.140]  So, then h sine theta is what?\n",
            "[01:00:03.140 --> 01:00:07.140]  Sine alpha is equal to y.\n",
            "[01:00:07.140 --> 01:00:11.140]  Y? Sorry, sine alpha.\n",
            "[01:00:11.140 --> 01:00:14.140]  Sine alpha would be the answer.\n",
            "[01:00:14.140 --> 01:00:20.140]  Right. Now, similarly for the triangle OBD, what can I write?\n",
            "[01:00:20.140 --> 01:00:25.140]  H cos alpha plus theta is equal to x dash.\n",
            "[01:00:25.140 --> 01:00:33.140]  H cos alpha plus theta is what? X dash?\n",
            "[01:00:33.140 --> 01:00:37.140]  X dash.\n",
            "[01:00:37.140 --> 01:00:43.140]  Right. And similarly, h.\n",
            "[01:00:43.140 --> 01:00:46.140]  Sine alpha plus theta is equal to y.\n",
            "[01:00:46.140 --> 01:00:54.140]  Sine alpha plus theta.\n",
            "[01:00:54.140 --> 01:00:58.140]  What is it? Y dash?\n",
            "[01:00:58.140 --> 01:01:06.140]  Now, what is cos a plus cos b or cos alpha plus cos theta? Cos alpha plus theta? Cos a plus b?\n",
            "[01:01:06.140 --> 01:01:09.140]  There is a formula, if you remember.\n",
            "[01:01:09.140 --> 01:01:13.140]  Sine a plus b is what? Or sine alpha plus theta is what?\n",
            "[01:01:13.140 --> 01:01:19.140]  Sine alpha into sine theta plus cos alpha into sine alpha.\n",
            "[01:01:19.140 --> 01:01:22.140]  Similarly, cos a plus b?\n",
            "[01:01:22.140 --> 01:01:30.140]  Cos a into sine b minus sine into cos b.\n",
            "[01:01:30.140 --> 01:01:32.140]  Okay. Whatever is the formula?\n",
            "[01:01:32.140 --> 01:01:38.140]  Expand this.\n",
            "[01:01:38.140 --> 01:01:45.140]  Okay. Now, let's write this. H sine alpha plus theta. What is sine a plus b? Sine a into cos b plus?\n",
            "[01:01:45.140 --> 01:01:49.140]  For sine it was that or cos it was that?\n",
            "[01:01:49.140 --> 01:01:55.140]  Yeah, I think for sine. Sine a plus b, sine a into cos b plus cos a into sine b. Isn't it?\n",
            "[01:01:55.140 --> 01:01:59.140]  Anybody can recall?\n",
            "[01:01:59.140 --> 01:02:04.140]  Yes, sir. Sine a cos b plus sine b cos a.\n",
            "[01:02:04.140 --> 01:02:08.140]  For sine alpha plus beta, right?\n",
            "[01:02:08.140 --> 01:02:09.140]  Correct.\n",
            "[01:02:09.140 --> 01:02:16.140]  Sine alpha, H times sine alpha, right?\n",
            "[01:02:16.140 --> 01:02:19.140]  Sine alpha plus sine theta?\n",
            "[01:02:19.140 --> 01:02:28.140]  That's which one you are expanding. Sine alpha plus theta or cos alpha plus theta?\n",
            "[01:02:28.140 --> 01:02:33.140]  Sine alpha plus theta would be sine alpha cos theta.\n",
            "[01:02:33.140 --> 01:02:40.140]  Cos theta plus sine theta.\n",
            "[01:02:40.140 --> 01:02:44.140]  Then H will go inside again because it's expansion of this.\n",
            "[01:02:44.140 --> 01:02:50.140]  Plus sine theta into cos alpha.\n",
            "[01:02:50.140 --> 01:02:55.140]  Cos alpha, right? This is theta, right?\n",
            "[01:02:55.140 --> 01:03:01.140]  That's y dash, right?\n",
            "[01:03:01.140 --> 01:03:06.140]  Which I can write as H sine alpha is what?\n",
            "[01:03:06.140 --> 01:03:09.140]  This is H sine alpha.\n",
            "[01:03:09.140 --> 01:03:17.140]  So, I can write as y cos theta plus what is H cos theta?\n",
            "[01:03:17.140 --> 01:03:19.140]  X.\n",
            "[01:03:19.140 --> 01:03:29.140]  So, I can write plus x sine theta is equal to y dash.\n",
            "[01:03:29.140 --> 01:03:32.140]  This is the same equation which is written here.\n",
            "[01:03:32.140 --> 01:03:33.140]  Yes, sir.\n",
            "[01:03:33.140 --> 01:03:37.140]  Similarly, it can be also evaluated, computed.\n",
            "[01:03:37.140 --> 01:03:41.140]  And now these two equations can be written in a matrix form like this.\n",
            "[01:03:41.140 --> 01:03:51.140]  So, x dash, y dash vector is equal to cos theta minus sine theta, sine theta cos theta and x, y on the right side as I'm multiplying to this matrix.\n",
            "[01:03:51.140 --> 01:03:55.140]  So, this matrix becomes rotation matrix R.\n",
            "[01:03:55.140 --> 01:04:04.140]  And this becomes a 2D point P. Before rotation, this becomes coordinate of point P after rotation called P dash.\n",
            "[01:04:04.140 --> 01:04:09.140]  So, this is a rotation matrix formulation.\n",
            "[01:04:09.140 --> 01:04:14.140]  So, every pixel I will rotate with the same rotation matrix for a given theta.\n",
            "[01:04:14.140 --> 01:04:18.140]  That will give me new coordinates.\n",
            "[01:04:18.140 --> 01:04:24.140]  So, for every x, y, I can compute x dash, y dash after rotation and I can just copy the values.\n",
            "[01:04:24.140 --> 01:04:30.140]  So, that will give me rotation of the image.\n",
            "[01:04:30.140 --> 01:04:33.140]  Just for completeness, I'm telling you a few things.\n",
            "[01:04:33.140 --> 01:04:39.140]  This rotation matrix is orthonormal matrix with determinant 1.\n",
            "[01:04:39.140 --> 01:04:42.140]  What will be the determinant of this matrix?\n",
            "[01:04:42.140 --> 01:04:50.140]  It's a 2D matrix. The determinant will be cos square theta diagonal elements, multiplication minus, minus of sine square theta.\n",
            "[01:04:50.140 --> 01:04:55.140]  That will become plus the cos square theta plus sine square theta, which will be 1.\n",
            "[01:04:55.140 --> 01:04:58.140]  So, determinant of rotation matrix is always 1.\n",
            "[01:04:58.140 --> 01:05:03.140]  And transpose of a rotation matrix is inverse itself.\n",
            "[01:05:03.140 --> 01:05:08.140]  So, when I say rotate by theta, that will be R, R theta matrix.\n",
            "[01:05:08.140 --> 01:05:12.140]  I typically represent this as not just R, but parameterized by theta.\n",
            "[01:05:12.140 --> 01:05:16.140]  Either below or I can put in a bracket.\n",
            "[01:05:16.140 --> 01:05:21.140]  And this rotation is always convention is counterclockwise for positive theta.\n",
            "[01:05:21.140 --> 01:05:25.140]  So, if I say rotate by minus theta, that means rotate rightward.\n",
            "[01:05:25.140 --> 01:05:30.140]  Clockwise rotation, if you want, you have to feed minus theta.\n",
            "[01:05:30.140 --> 01:05:34.140]  Anti-clockwise, you have to make it plus theta, counterclockwise.\n",
            "[01:05:34.140 --> 01:05:41.140]  You have to make it plus theta.\n",
            "[01:05:41.140 --> 01:05:45.140]  Is that clear to everyone?\n",
            "[01:05:45.140 --> 01:05:49.140]  Yes, sir. Yes, sir.\n",
            "[01:05:49.140 --> 01:05:56.140]  Sir, so if we have to rotate it clockwise, so we have to take transpose of this rotation matrix?\n",
            "[01:05:56.140 --> 01:05:59.140]  No, no, no. You have to just feed theta as minus.\n",
            "[01:05:59.140 --> 01:06:04.140]  Cos of minus theta will become, what is cos of minus theta?\n",
            "[01:06:04.140 --> 01:06:07.140]  What is sine of? Cos of minus theta is theta itself, I think.\n",
            "[01:06:07.140 --> 01:06:10.140]  Sine of minus theta will become minus sine theta.\n",
            "[01:06:10.140 --> 01:06:18.140]  Or sine of minus pi by 4, so that we have different value, cos of minus pi by 4.\n",
            "[01:06:18.140 --> 01:06:22.140]  So, if you have to rotate the image rightward, you have to put minus theta.\n",
            "[01:06:22.140 --> 01:06:26.140]  If you rotate the image leftward, you have to put plus theta.\n",
            "[01:06:26.140 --> 01:06:27.140]  Okay, yes, sir.\n",
            "[01:06:27.140 --> 01:06:33.140]  I think cos and sine of minus theta will be 270 minus and all, whatever is the function.\n",
            "[01:06:33.140 --> 01:06:40.140]  Okay, so rotation is simple.\n",
            "[01:06:40.140 --> 01:06:48.140]  Okay, so this is an example, the same matrix just with alpha I am writing instead of theta, so don't get confused.\n",
            "[01:06:48.140 --> 01:06:51.140]  Okay, so this is a rotation.\n",
            "[01:06:51.140 --> 01:06:54.140]  Here it will be minus alpha, okay.\n",
            "[01:06:54.140 --> 01:07:00.140]  Alpha will be some negative angle, that's all I am saying. If I rotate rightward.\n",
            "[01:07:00.140 --> 01:07:06.140]  Okay, now there are other simple operations like image flipping.\n",
            "[01:07:06.140 --> 01:07:09.140]  Right, so image flipping is easy.\n",
            "[01:07:09.140 --> 01:07:18.140]  If I had to flip vertically like what is shown here at the bottom, you just take x, y is mapped to m minus x, y.\n",
            "[01:07:18.140 --> 01:07:21.140]  So, I don't need to apply rotation matrix of 180 degree.\n",
            "[01:07:21.140 --> 01:07:25.140]  I have to just apply transformation on x itself or y itself.\n",
            "[01:07:25.140 --> 01:07:33.140]  Similarly, I can do horizontal flip with flipping the y axis.\n",
            "[01:07:33.140 --> 01:07:41.140]  Instead of y being, only y I will make it n minus y, n is the number of pixels along y axis.\n",
            "[01:07:41.140 --> 01:07:45.140]  Similarly, m is a pixel around x axis, n number of pixels.\n",
            "[01:07:45.140 --> 01:07:49.140]  Right, this is a simple operation.\n",
            "[01:07:49.140 --> 01:07:56.140]  Nothing is changing, just the location is I am switching along x or along y axis. Is that clear?\n",
            "[01:07:56.140 --> 01:07:59.140]  It's like translation, sir.\n",
            "[01:08:00.140 --> 01:08:03.140]  No, this is just switching the values.\n",
            "[01:08:03.140 --> 01:08:10.140]  There's no, see x is remaining same for the flip around horizontal, y is remaining same for the vertical.\n",
            "[01:08:10.140 --> 01:08:13.140]  If you see in this image, this is a vertical flipping.\n",
            "[01:08:13.140 --> 01:08:17.140]  That means I am only changing the x coordinate.\n",
            "[01:08:17.140 --> 01:08:28.140]  So, a pixel here, right, let me draw a green color dot here, is now the pixel location here for the same y.\n",
            "[01:08:28.140 --> 01:08:33.140]  And a pixel here is now shifted here.\n",
            "[01:08:33.140 --> 01:08:38.140]  This is transformed here, this is transformed here for the same y.\n",
            "[01:08:38.140 --> 01:08:46.140]  Okay, that is only changing the location along x.\n",
            "[01:08:46.140 --> 01:08:52.140]  So, there's no translation, it's flipping.\n",
            "[01:08:52.140 --> 01:08:56.140]  It's like a mirror image, sir.\n",
            "[01:08:56.140 --> 01:08:59.140]  Mirror image, yes.\n",
            "[01:08:59.140 --> 01:09:05.140]  Along vertical x or I can also do horizontally, I can also make this flip horizontally.\n",
            "[01:09:05.140 --> 01:09:09.140]  I can make the cap bottom and the body on the top.\n",
            "[01:09:09.140 --> 01:09:13.140]  Okay.\n",
            "[01:09:13.140 --> 01:09:16.140]  Okay, easy.\n",
            "[01:09:16.140 --> 01:09:24.140]  Now, nobody has asked that question till now, especially in rotations, let me flip that.\n",
            "[01:09:24.140 --> 01:09:33.140]  When I am rotating the image, the x dash y dash that I will get, am I guaranteed to get them as integers?\n",
            "[01:09:33.140 --> 01:09:39.140]  Because pixel locations are integers, yes or no?\n",
            "[01:09:39.140 --> 01:09:43.140]  Pixel locations are integers.\n",
            "[01:09:43.140 --> 01:09:51.140]  But does the cos alpha and sin alpha values will always be integer or their summation or subtraction?\n",
            "[01:09:51.140 --> 01:10:02.140]  No, I might not map after transformation, the x dash y dash may not be valid pixel locations.\n",
            "[01:10:02.140 --> 01:10:07.140]  There will be subpixels, somewhere inside the pixel it will map.\n",
            "[01:10:07.140 --> 01:10:19.140]  Like if the input is 3 comma 4, after transformation I might get 6.8 and 7.5.\n",
            "[01:10:19.140 --> 01:10:23.140]  Right?\n",
            "[01:10:23.140 --> 01:10:25.140]  So this is the problem.\n",
            "[01:10:25.140 --> 01:10:35.140]  After transformation, x, y might project to somewhere in the middle of pixel or not necessarily even middle.\n",
            "[01:10:35.140 --> 01:10:37.140]  Right?\n",
            "[01:10:37.140 --> 01:10:44.140]  So if you really see, if I come take the, this is just a C depiction of a code.\n",
            "[01:10:44.140 --> 01:10:51.140]  If I apply a transformation on x coordinate by fx and transformation function of y coordinate by fy,\n",
            "[01:10:51.140 --> 01:10:56.140]  nothing but the first row of rotation matrix and second row of rotation matrix as an example,\n",
            "[01:10:56.140 --> 01:11:01.140]  then x dash y dash will be float, they need not to be integer.\n",
            "[01:11:01.140 --> 01:11:02.140]  Right?\n",
            "[01:11:02.140 --> 01:11:06.140]  It will be some real value with decimal.\n",
            "[01:11:06.140 --> 01:11:08.140]  It might be, I mean.\n",
            "[01:11:08.140 --> 01:11:09.140]  Okay?\n",
            "[01:11:09.140 --> 01:11:12.140]  For example, translation, it will not be necessarily.\n",
            "[01:11:12.140 --> 01:11:16.140]  If I'm just translating by integers, okay?\n",
            "[01:11:16.140 --> 01:11:18.140]  Along x and y.\n",
            "[01:11:18.140 --> 01:11:24.140]  But rotation, typically it is there, shearing it might be there.\n",
            "[01:11:24.140 --> 01:11:26.140]  Right?\n",
            "[01:11:26.140 --> 01:11:28.140]  Excuse me sir.\n",
            "[01:11:28.140 --> 01:11:30.140]  I'm getting confused here.\n",
            "[01:11:30.140 --> 01:11:35.140]  So rotating the image means rotating the coordinate of that pixels, right?\n",
            "[01:11:35.140 --> 01:11:36.140]  Yeah.\n",
            "[01:11:36.140 --> 01:11:39.140]  So, but value of the pixels will be the same, right?\n",
            "[01:11:39.140 --> 01:11:46.140]  For example, having some value related to from 0 to 255.\n",
            "[01:11:46.140 --> 01:11:47.140]  No, no, no.\n",
            "[01:11:47.140 --> 01:11:48.140]  Wait a minute.\n",
            "[01:11:48.140 --> 01:11:49.140]  I'm not talking about value here.\n",
            "[01:11:49.140 --> 01:11:51.140]  In this right side code is shown anyway.\n",
            "[01:11:51.140 --> 01:11:53.140]  Anywhere are you talking about the value at that point?\n",
            "[01:11:53.140 --> 01:11:54.140]  No, no, no sir.\n",
            "[01:11:54.140 --> 01:11:57.140]  So both are different, means different, different things.\n",
            "[01:11:57.140 --> 01:11:59.140]  I am just talking about the grid itself.\n",
            "[01:11:59.140 --> 01:12:03.140]  The original grid was uniform integer locations were there.\n",
            "[01:12:03.140 --> 01:12:08.140]  After transformation, I am mapping to non-integer values, right?\n",
            "[01:12:08.140 --> 01:12:09.140]  Okay, sir.\n",
            "[01:12:09.140 --> 01:12:14.140]  So if I just truncate the decimal, will it give me good image?\n",
            "[01:12:14.140 --> 01:12:15.140]  No, right?\n",
            "[01:12:15.140 --> 01:12:17.140]  Image will deform, right?\n",
            "[01:12:17.140 --> 01:12:19.140]  Okay.\n",
            "[01:12:19.140 --> 01:12:24.140]  See, I'll just try to give you an example.\n",
            "[01:12:24.140 --> 01:12:26.140]  Okay, sir.\n",
            "[01:12:26.140 --> 01:12:32.140]  If I take this same image and if I draw, let's say there's a line.\n",
            "[01:12:32.140 --> 01:12:35.140]  This pixel is part of a line, right?\n",
            "[01:12:35.140 --> 01:12:40.140]  Let's say the line is passing like this.\n",
            "[01:12:40.140 --> 01:12:48.140]  Okay, so when I do rasterization, I only store the color of this line at this point and this point and this point.\n",
            "[01:12:48.140 --> 01:12:51.140]  These are pixels which I will explain, right?\n",
            "[01:12:51.140 --> 01:12:57.140]  Or if I have a good scale and a good way to draw them, think of it, the line is actually this, right?\n",
            "[01:12:57.140 --> 01:13:03.140]  After transformation, the line should remain same, right?\n",
            "[01:13:03.140 --> 01:13:09.140]  So line should ideally draw on this particular grid, dotted grid, yes or no?\n",
            "[01:13:09.140 --> 01:13:10.140]  Yes, sir.\n",
            "[01:13:10.140 --> 01:13:14.140]  Line or unfortunately what I am drawing is a curve, but okay.\n",
            "[01:13:14.140 --> 01:13:19.140]  Now, what will happen to this line if I just truncate the decimal?\n",
            "[01:13:19.140 --> 01:13:22.140]  Will it remain line?\n",
            "[01:13:22.140 --> 01:13:25.140]  No, right? Decimal means what?\n",
            "[01:13:25.140 --> 01:13:30.140]  So dotted grid is what is desired, but what is reality is this lines which are there.\n",
            "[01:13:30.140 --> 01:13:33.140]  This is the my buffer grid, right?\n",
            "[01:13:33.140 --> 01:13:35.140]  Original grid, raster grid.\n",
            "[01:13:35.140 --> 01:13:40.140]  So this if I just do decimal point, then this point will be mapped to this.\n",
            "[01:13:40.140 --> 01:13:47.140]  This dotted point will be mapped to this point and then this dotted point will be mapped to the closest point.\n",
            "[01:13:47.140 --> 01:13:55.140]  Let's say this one and this one possibly here will be mapped to this point here.\n",
            "[01:13:55.140 --> 01:13:58.140]  So my line will not remain line anymore.\n",
            "[01:13:58.140 --> 01:13:59.140]  It will deform.\n",
            "[01:13:59.140 --> 01:14:03.140]  That means image will deform.\n",
            "[01:14:03.140 --> 01:14:12.140]  Which means transform point x dash y dash may not project to the valid exact grid points.\n",
            "[01:14:12.140 --> 01:14:19.140]  They fall somewhere in between and hence I need a way to tackle this problem.\n",
            "[01:14:19.140 --> 01:14:24.140]  So this was a forward mapping that I was talking about that we have been doing till now.\n",
            "[01:14:24.140 --> 01:14:29.140]  That matrix if I multiply x y to this matrix, I will get x dash y dash.\n",
            "[01:14:29.140 --> 01:14:38.140]  This x dash y dash need not to be integer values of valid grid locations.\n",
            "[01:14:38.140 --> 01:14:42.140]  So I need to figure out a way to deal with it.\n",
            "[01:14:42.140 --> 01:14:43.140]  Yes or no?\n",
            "[01:14:43.140 --> 01:14:44.140]  Yes, sir.\n",
            "[01:14:44.140 --> 01:14:46.140]  Problem is understood to everyone.\n",
            "[01:14:46.140 --> 01:14:48.140]  Anyone still have a confusion? Ask me.\n",
            "[01:14:48.140 --> 01:14:52.140]  No, sir.\n",
            "[01:14:52.140 --> 01:14:53.140]  Right, sir.\n",
            "[01:14:53.140 --> 01:15:03.140]  I'm thinking of a case like we are, let's say, rotating a joint which is, let's say,\n",
            "[01:15:03.140 --> 01:15:07.140]  or bending something from 90 degree angle to 120 degree angle.\n",
            "[01:15:07.140 --> 01:15:11.140]  And the way those curves are going to get deformed because of that.\n",
            "[01:15:11.140 --> 01:15:12.140]  Forget about curves.\n",
            "[01:15:12.140 --> 01:15:13.140]  Just take a line.\n",
            "[01:15:13.140 --> 01:15:20.140]  If you take a line and deform the line, right, you look at in this example in this image,\n",
            "[01:15:20.140 --> 01:15:25.140]  the dotted grids and the black grids on the right side.\n",
            "[01:15:25.140 --> 01:15:30.140]  The dotted grid is where this image will project.\n",
            "[01:15:30.140 --> 01:15:31.140]  Right.\n",
            "[01:15:31.140 --> 01:15:39.140]  But the reality is the black grids on which I will store the values that are the valid pixel location and integer.\n",
            "[01:15:39.140 --> 01:15:47.140]  So it means that the shape, the shape of all these four dots, which are forming, let's say, square or rectangle,\n",
            "[01:15:47.140 --> 01:15:51.140]  the shape will be same.\n",
            "[01:15:51.140 --> 01:15:52.140]  Shape should remain same.\n",
            "[01:15:52.140 --> 01:15:53.140]  That is desired.\n",
            "[01:15:53.140 --> 01:15:54.140]  Yeah.\n",
            "[01:15:54.140 --> 01:16:03.140]  Problem is this black dot here, let me use another red color or something.\n",
            "[01:16:03.140 --> 01:16:08.140]  The black dot here, right, this one, it is not falling on any pixel location.\n",
            "[01:16:08.140 --> 01:16:14.140]  The closest pixel is either this or this.\n",
            "[01:16:14.140 --> 01:16:18.140]  Isn't it?\n",
            "[01:16:18.140 --> 01:16:19.140]  Yeah, I understood, sir.\n",
            "[01:16:19.140 --> 01:16:21.140]  That point understood.\n",
            "[01:16:21.140 --> 01:16:25.140]  So then how do I deal with this problem?\n",
            "[01:16:25.140 --> 01:16:33.140]  Because ultimately the image intensities will be stored at these locations.\n",
            "[01:16:33.140 --> 01:16:36.140]  Solution can be reduced the grid size a lot, sir.\n",
            "[01:16:36.140 --> 01:16:38.140]  These blocks which we are saying.\n",
            "[01:16:38.140 --> 01:16:44.140]  No, we cannot because that is practically saying that I am trying to change the resolution of the image.\n",
            "[01:16:44.140 --> 01:16:57.140]  If you'd increase more grid size by oversampling or oscillating the space of grid, densifying them.\n",
            "[01:16:57.140 --> 01:16:58.140]  That's what you're saying.\n",
            "[01:16:58.140 --> 01:16:59.140]  Right.\n",
            "[01:16:59.140 --> 01:17:01.140]  Then eventually, how do you fill information?\n",
            "[01:17:01.140 --> 01:17:02.140]  That's one problem.\n",
            "[01:17:02.140 --> 01:17:04.140]  You will interpolate.\n",
            "[01:17:04.140 --> 01:17:09.140]  The second problem will be your storage will be image will become bigger to store.\n",
            "[01:17:09.140 --> 01:17:16.140]  If you increase by two itself, it will make it much bigger.\n",
            "[01:17:16.140 --> 01:17:17.140]  Right.\n",
            "[01:17:17.140 --> 01:17:31.140]  If I had 100 by 100 image, 100 pixel by 100, and if I increase the resolution by 10 to compensate for decimal point of 0.1, 0.2, 0.3,\n",
            "[01:17:31.140 --> 01:17:34.140]  I will say add 10 grids in each grid.\n",
            "[01:17:34.140 --> 01:17:36.140]  Isn't it?\n",
            "[01:17:36.140 --> 01:17:39.140]  10 more cells along X, 10 more cells along Y.\n",
            "[01:17:39.140 --> 01:17:42.140]  That will increase the resolution 100 times.\n",
            "[01:17:42.140 --> 01:17:44.140]  The 2 MB image will become 200 MB.\n",
            "[01:17:44.140 --> 01:17:46.140]  Yes or no?\n",
            "[01:17:46.140 --> 01:17:47.140]  Yeah, you're right, sir.\n",
            "[01:17:47.140 --> 01:17:51.140]  Which we don't want to do because we can't afford that.\n",
            "[01:17:51.140 --> 01:17:52.140]  It's not a principal solution.\n",
            "[01:17:52.140 --> 01:17:54.140]  Then again, the problem will persist.\n",
            "[01:17:54.140 --> 01:17:57.140]  There here you are doing a decimal.\n",
            "[01:17:57.140 --> 01:18:05.140]  If you increase the grids or grid, the grid resolution by 10 along X and Y, all you're dealing is the first decimal point.\n",
            "[01:18:05.140 --> 01:18:06.140]  What about second decimal then?\n",
            "[01:18:06.140 --> 01:18:08.140]  You're truncating that, right?\n",
            "[01:18:08.140 --> 01:18:13.140]  X dash Y dash could be 1.23, 1.58.\n",
            "[01:18:13.140 --> 01:18:15.140]  Right.\n",
            "[01:18:15.140 --> 01:18:20.140]  So what you're saying is a possible solution, but not affordable.\n",
            "[01:18:20.140 --> 01:18:21.140]  Yeah, yeah.\n",
            "[01:18:21.140 --> 01:18:22.140]  Got it, sir.\n",
            "[01:18:22.140 --> 01:18:23.140]  Thank you so much.\n",
            "[01:18:23.140 --> 01:18:29.140]  Then the thing should be like we must consider the nearest pixel point and optimize for that.\n",
            "[01:18:29.140 --> 01:18:32.140]  That will deform the image content.\n",
            "[01:18:32.140 --> 01:18:35.140]  Is that it?\n",
            "[01:18:35.140 --> 01:18:38.140]  Then the line will not remain line.\n",
            "[01:18:38.140 --> 01:18:41.140]  It will get staircase kind of effect.\n",
            "[01:18:41.140 --> 01:18:43.140]  Right.\n",
            "[01:18:43.140 --> 01:18:44.140]  Right.\n",
            "[01:18:44.140 --> 01:18:48.140]  Anyone else who has the idea of?\n",
            "[01:18:48.140 --> 01:18:51.140]  Calculating the neighboring filters.\n",
            "[01:18:51.140 --> 01:18:58.140]  So, I mean, you have to keep some value of neighbor and then calculate accordingly somehow.\n",
            "[01:18:58.140 --> 01:19:03.140]  No, the idea is little bit more simple than that.\n",
            "[01:19:03.140 --> 01:19:07.140]  Algorithmically, you have to think little bit.\n",
            "[01:19:07.140 --> 01:19:08.140]  Right.\n",
            "[01:19:08.140 --> 01:19:13.140]  So essentially, this is the practical problem that when you rotate.\n",
            "[01:19:13.140 --> 01:19:18.140]  So here again, from previous slide, there's a slight change of convention.\n",
            "[01:19:18.140 --> 01:19:24.140]  If I slide the black dots on the pixel locations here, I'm literally drawing each cell as a pixel.\n",
            "[01:19:24.140 --> 01:19:33.140]  Each circle here is representing that cell, that square grid that I'm calling as pixel.\n",
            "[01:19:33.140 --> 01:19:40.140]  If I rotate it, what is happening is either more than one pixel is being projected to a single cell.\n",
            "[01:19:40.140 --> 01:19:42.140]  That's the case.\n",
            "[01:19:42.140 --> 01:19:51.140]  If two pixels are part of the cell or really look at certain pixels might have no projection.\n",
            "[01:19:51.140 --> 01:19:54.140]  Even though it's part of the image, it is blank.\n",
            "[01:19:54.140 --> 01:19:56.140]  One cell is blank.\n",
            "[01:19:56.140 --> 01:20:01.140]  So we don't know from where we get the appearance of this pixel in the transformed image.\n",
            "[01:20:01.140 --> 01:20:05.140]  And here I have to interpolate or do averaging or something like that.\n",
            "[01:20:05.140 --> 01:20:11.140]  So one simple solution to this problem is you try to do the reverse mapping.\n",
            "[01:20:11.140 --> 01:20:14.140]  Right.\n",
            "[01:20:14.140 --> 01:20:23.140]  Which basically means if you have to rotate the image by 30 degree, you think of I'm rotating the rotated image back to original image.\n",
            "[01:20:23.140 --> 01:20:25.140]  Right.\n",
            "[01:20:25.140 --> 01:20:35.140]  So now for every valid pixel of this image, after applying rotation of minus 30, I will know where it will project in this original image.\n",
            "[01:20:35.140 --> 01:20:40.140]  And in the original image, I can take the intensities and I can interpolate.\n",
            "[01:20:40.140 --> 01:20:43.140]  How can I interpolate?\n",
            "[01:20:43.140 --> 01:20:48.140]  So what I'm saying is first look at this code, piece of code.\n",
            "[01:20:48.140 --> 01:20:52.140]  I'm applying an inverse transformation on X dash Y dash.\n",
            "[01:20:52.140 --> 01:20:54.140]  So now X dash Y dash is strictly a grid.\n",
            "[01:20:54.140 --> 01:20:57.140]  X dash Y dash are integer locations of pixels.\n",
            "[01:20:57.140 --> 01:20:58.140]  Right.\n",
            "[01:20:58.140 --> 01:20:59.140]  Which I need to fill.\n",
            "[01:20:59.140 --> 01:21:01.140]  This is the image I don't know.\n",
            "[01:21:01.140 --> 01:21:06.140]  I want to fill this image with intensity values from the original source image.\n",
            "[01:21:06.140 --> 01:21:13.140]  So I will take every pixel of this image and I will try to find the XY float location in the original image.\n",
            "[01:21:13.140 --> 01:21:16.140]  Right.\n",
            "[01:21:16.140 --> 01:21:27.140]  So in the original image, XY need not to be, the XY that I will recover will need not to be valid pixel location, but I will find some pixel locations, some decimal locations.\n",
            "[01:21:27.140 --> 01:21:28.140]  Okay.\n",
            "[01:21:28.140 --> 01:21:30.140]  That's why I'm using float to store that.\n",
            "[01:21:30.140 --> 01:21:37.140]  And then I need a way to copy the appearance from that original image to this new image.\n",
            "[01:21:37.140 --> 01:21:44.140]  So DST of X dash Y dash that copying is not that easy because XY need not to be integer location.\n",
            "[01:21:44.140 --> 01:21:47.140]  So I need some resampling of the source image.\n",
            "[01:21:47.140 --> 01:21:49.140]  I need another function.\n",
            "[01:21:49.140 --> 01:21:50.140]  Right.\n",
            "[01:21:50.140 --> 01:21:55.140]  Which give given X and Y which are float and some W, some mask or some weights.\n",
            "[01:21:55.140 --> 01:22:01.140]  And then I need to compute the appearance of the target image pixel.\n",
            "[01:22:01.140 --> 01:22:05.140]  So target image pixel, I am doing an iteration for every pixel of a target image.\n",
            "[01:22:05.140 --> 01:22:10.140]  I am trying to find what is the source image from where I have to borrow the appearance.\n",
            "[01:22:10.140 --> 01:22:12.140]  How do we do that?\n",
            "[01:22:12.140 --> 01:22:13.140]  This is how we do that.\n",
            "[01:22:13.140 --> 01:22:21.140]  So if you see U comma V is the reverse mapped image on the source pixel.\n",
            "[01:22:21.140 --> 01:22:26.140]  So the X comma Y of the target image, sorry, X dash Y dash of the target image.\n",
            "[01:22:26.140 --> 01:22:32.140]  Let's say it's mapping to U comma V in the source image after applying S inverse reverse mapping.\n",
            "[01:22:32.140 --> 01:22:37.140]  Then I need to find the appearance of this U comma V.\n",
            "[01:22:37.140 --> 01:22:38.140]  Right.\n",
            "[01:22:38.140 --> 01:22:43.140]  And these are the four valid pixel locations which will influence the appearance of this.\n",
            "[01:22:43.140 --> 01:22:46.140]  So I will interpolate on this either with a Gaussian mask.\n",
            "[01:22:46.140 --> 01:22:49.140]  So the circle and Gaussian interpolations.\n",
            "[01:22:49.140 --> 01:22:54.140]  So, or sorry, or I can do a linear interpolation or called bilinear interpolation.\n",
            "[01:22:54.140 --> 01:23:01.140]  So I can say appearance of U1 V1, U2 V2 will give me appearance of A along this cell.\n",
            "[01:23:01.140 --> 01:23:02.140]  Edge of the cell.\n",
            "[01:23:02.140 --> 01:23:06.140]  Some problem with the mouse, the way it operates.\n",
            "[01:23:06.140 --> 01:23:07.140]  Okay.\n",
            "[01:23:07.140 --> 01:23:15.140]  And similarly for the U1 V1 pixel and U2 V1 pixel, I will get the appearance of B along this.\n",
            "[01:23:15.140 --> 01:23:19.140]  And I will interpolate along Y axis to get the UV appearance.\n",
            "[01:23:19.140 --> 01:23:21.140]  That's called bilinear interpolation.\n",
            "[01:23:21.140 --> 01:23:29.140]  So practically what I'm saying is the original problem that I had was when I take the XY location of the image\n",
            "[01:23:29.140 --> 01:23:36.140]  and project it into new image grid, I am not getting the integer locations where this source pixel will map to.\n",
            "[01:23:36.140 --> 01:23:40.140]  So to deal with it, I am doing the other way around.\n",
            "[01:23:40.140 --> 01:23:47.140]  I am taking that my image that I need to fill is the target image.\n",
            "[01:23:47.140 --> 01:23:54.140]  So for every location of the target image, I am trying to find which source image location it falls to.\n",
            "[01:23:54.140 --> 01:23:59.140]  And this source image location it falls to need not to be integer location.\n",
            "[01:23:59.140 --> 01:24:04.140]  And for that, I am essentially doing an interpolation on the source image.\n",
            "[01:24:04.140 --> 01:24:09.140]  Where all pixels, I have the location known.\n",
            "[01:24:09.140 --> 01:24:14.140]  So I can interpolate the sub pixel locations and I can fill the target image.\n",
            "[01:24:14.140 --> 01:24:21.140]  That will give me basically this kind of transformation successfully done like rotation here.\n",
            "[01:24:22.140 --> 01:24:25.140]  I can now rotate.\n",
            "[01:24:25.140 --> 01:24:29.140]  So what essentially I'm doing is let me show it here in this slide.\n",
            "[01:24:35.140 --> 01:24:41.140]  What I'm doing here is I am taking each pixel of this image one by one.\n",
            "[01:24:41.140 --> 01:24:45.140]  So I am sampling every pixel.\n",
            "[01:24:45.140 --> 01:24:53.140]  And for each pixel of this image, I am trying to map with reverse mapping that which pixel of the image I will get.\n",
            "[01:24:53.140 --> 01:24:59.140]  Now here the problem is when I map back, I might not get integer value.\n",
            "[01:24:59.140 --> 01:25:05.140]  So if I take this cell here, let's say a very big representation of that.\n",
            "[01:25:05.140 --> 01:25:12.140]  Then I might be mapping somewhere here.\n",
            "[01:25:13.140 --> 01:25:22.140]  For a valid pixel, integer pixel here x, y, I am actually mapping to this position in the middle of the cell somewhere.\n",
            "[01:25:22.140 --> 01:25:29.140]  And what I am storing is the appearance of this circle, this circle, this circle and this circle.\n",
            "[01:25:29.140 --> 01:25:37.140]  So I will interpolate this value either with a Gaussian kernel or I will interpolate with bilinear interpolation.\n",
            "[01:25:37.140 --> 01:25:40.140]  So I will take averaging along this line.\n",
            "[01:25:40.140 --> 01:25:44.140]  So I get this value of the intensity, this value of the intensity.\n",
            "[01:25:44.140 --> 01:25:49.140]  Then I will average along this and I will get this value.\n",
            "[01:25:52.140 --> 01:25:58.140]  You are saying that the storage amount of that image would increase because of it?\n",
            "[01:25:58.140 --> 01:26:04.140]  No, nothing will increase because the resolution of this image is the same.\n",
            "[01:26:04.140 --> 01:26:07.140]  Maybe I am canvassing with bigger.\n",
            "[01:26:07.140 --> 01:26:13.140]  So if this was original image was 100 by 100, this might be 120 by 120. That's all.\n",
            "[01:26:13.140 --> 01:26:20.140]  But since we are dealing with four pixel at a time to store that.\n",
            "[01:26:20.140 --> 01:26:23.140]  This is a conversational cost.\n",
            "[01:26:23.140 --> 01:26:33.140]  When I map that to this pixel here, mapping to some pixel with decimal point as let's say along x it is.\n",
            "[01:26:33.140 --> 01:26:42.140]  So if it was 10 here, it might be 8 here or 7 here point along x-axis.\n",
            "[01:26:42.140 --> 01:26:46.140]  Let's say it is 8.3 or 8.5.\n",
            "[01:26:46.140 --> 01:26:49.140]  Let's make it easy the way I am depicting it.\n",
            "[01:26:49.140 --> 01:26:56.140]  And along y-axis, it might be 7.3.\n",
            "[01:26:56.140 --> 01:27:00.140]  Here it was some 10 plus whatever 11 or something.\n",
            "[01:27:00.140 --> 01:27:05.140]  So for this location here, I want to know what is the appearance I have to copy from which position.\n",
            "[01:27:05.140 --> 01:27:09.140]  This position 8.5 and 7.3.\n",
            "[01:27:09.140 --> 01:27:14.140]  Now the 8.5, how will I get the appearance of this 8.5?\n",
            "[01:27:14.140 --> 01:27:20.140]  I will get the appearance by making 4 pixels I will take which is 8.9.\n",
            "[01:27:20.140 --> 01:27:24.140]  So it's about calculation on that.\n",
            "[01:27:24.140 --> 01:27:27.140]  I might divide by 2 middle point.\n",
            "[01:27:27.140 --> 01:27:31.140]  And similarly, 0.3 influence I want to get.\n",
            "[01:27:31.140 --> 01:27:36.140]  If it is 0.3, then it's closer to 8, closer to 7 than 8.\n",
            "[01:27:36.140 --> 01:27:39.140]  So I will appropriately do that.\n",
            "[01:27:39.140 --> 01:27:41.140]  Are you getting my point?\n",
            "[01:27:41.140 --> 01:27:45.140]  So basically it's a calculation to get those details.\n",
            "[01:27:45.140 --> 01:27:49.140]  So it's about calculation on the source image.\n",
            "[01:27:49.140 --> 01:27:53.140]  Rather than over sampling the target image.\n",
            "[01:27:53.140 --> 01:28:06.140]  So as I understood, sir, we need to have the intensity value first of that particular image, all the values.\n",
            "[01:28:06.140 --> 01:28:09.140]  Then we are rotating, let's say, by...\n",
            "[01:28:09.140 --> 01:28:12.140]  Now call it source or target.\n",
            "[01:28:12.140 --> 01:28:15.140]  Source image you already know, target image you have to fill.\n",
            "[01:28:15.140 --> 01:28:21.140]  So for target image we are doing first rotation, let's say, of theta.\n",
            "[01:28:21.140 --> 01:28:25.140]  Then again reverse rotation of minus theta and putting those values.\n",
            "[01:28:25.140 --> 01:28:31.140]  So we don't need to do forward rotation anyway because all we are interested in is reverse rotation.\n",
            "[01:28:31.140 --> 01:28:34.140]  We know that theta R matrix is known to us, right?\n",
            "[01:28:34.140 --> 01:28:39.140]  And R inverse is nothing but transpose of R matrix.\n",
            "[01:28:39.140 --> 01:28:42.140]  R matrix is only 2 by 2 matrix.\n",
            "[01:28:42.140 --> 01:28:45.140]  Its inverse is transpose of this matrix.\n",
            "[01:28:45.140 --> 01:28:53.140]  That matrix you take and you apply to X coordinate, the first row transformation, second row of the row matrix, you'll apply to Y coordinate.\n",
            "[01:28:53.140 --> 01:28:58.140]  So for every X dash, Y dash, you will get an XY value.\n",
            "[01:28:58.140 --> 01:29:02.140]  That XY value, which is called reverse mapping, is on the source image.\n",
            "[01:29:02.140 --> 01:29:04.140]  So that is float.\n",
            "[01:29:04.140 --> 01:29:06.140]  That is no more integer.\n",
            "[01:29:06.140 --> 01:29:12.140]  So you will do interpolation on the source image and get the intensity value of the target pixel of the, which is rotated image.\n",
            "[01:29:12.140 --> 01:29:18.140]  So you are filling the rotated image in single shot by doing interpolation of the source image.\n",
            "[01:29:18.140 --> 01:29:25.140]  So literally this is what you are doing when you are doing warping or rotation of this kind.\n",
            "[01:29:25.140 --> 01:29:29.140]  This particular loop you will invoke.\n",
            "[01:29:29.140 --> 01:29:31.140]  Hello sir.\n",
            "[01:29:31.140 --> 01:29:37.140]  Let me ask, is this clear to those who asked this question?\n",
            "[01:29:37.140 --> 01:29:41.140]  To whom I was explaining that X dash, Y dash is what I am filling.\n",
            "[01:29:41.140 --> 01:29:44.140]  Only this loop I will run. That's all.\n",
            "[01:29:50.140 --> 01:29:52.140]  Is this clear to whoever was asking?\n",
            "[01:29:52.140 --> 01:29:55.140]  Sir, it was clear earlier.\n",
            "[01:29:55.140 --> 01:30:01.140]  I was asking something.\n",
            "[01:30:01.140 --> 01:30:03.140]  Yes, sir.\n",
            "[01:30:03.140 --> 01:30:17.140]  So isn't the averaging the pixels, like we are taking, we are giving, you know, taking the Gaussian or the bilinear.\n",
            "[01:30:17.140 --> 01:30:24.140]  Like we are getting the final intensity for one pixel from four pixels.\n",
            "[01:30:24.140 --> 01:30:31.140]  So aren't we, you know, averaging the minute details or the low frequency detail?\n",
            "[01:30:31.140 --> 01:30:35.140]  It is not significantly transformed.\n",
            "[01:30:35.140 --> 01:30:38.140]  Yes, there is some interpolation is happening.\n",
            "[01:30:38.140 --> 01:30:50.140]  So what we can always assume that the loss or the perturbation of information that is happening, it is very minute because there is no other way out.\n",
            "[01:30:50.140 --> 01:30:52.140]  You cannot keep it completely loss free.\n",
            "[01:30:52.140 --> 01:31:01.140]  If you have to do it completely loss free, then also there is a problem that unfilled areas, how will you fill in the new image?\n",
            "[01:31:01.140 --> 01:31:09.140]  If you say, let's say I cannot, I don't want to afford this interpolation on the source, I will do an over-gridding on the target.\n",
            "[01:31:09.140 --> 01:31:14.140]  One problem is what resolution you will over-grid.\n",
            "[01:31:14.140 --> 01:31:22.140]  As I said, even if you do a 10 grid resolution around X and Y, it becomes 100 times bigger image in terms of grid size.\n",
            "[01:31:22.140 --> 01:31:25.140]  And then also there will be many grids which will be unfilled.\n",
            "[01:31:25.140 --> 01:31:28.140]  In that case, you might have to any way interpolate.\n",
            "[01:31:29.140 --> 01:31:31.140]  This is a trade-off.\n",
            "[01:31:31.140 --> 01:31:35.140]  If you want to apply the transformation, you have to manipulate the local appearance a little bit.\n",
            "[01:31:35.140 --> 01:31:40.140]  But that's typically so small that human eye cannot easily decipher.\n",
            "[01:31:40.140 --> 01:31:45.140]  Why? Because the grid resolution is typically 10 raised to the power 6 pixels.\n",
            "[01:31:45.140 --> 01:31:47.140]  It is anyway very small.\n",
            "[01:31:47.140 --> 01:31:53.140]  Unless you zoom the image and see that sub-pixel resolution, then only you will see that difference in intensities.\n",
            "[01:31:53.140 --> 01:31:55.140]  Naked eye you cannot see typically.\n",
            "[01:31:55.140 --> 01:32:00.140]  If you have a reasonably good image like 1 megapixel or even 500 by 500 image.\n",
            "[01:32:00.140 --> 01:32:09.140]  Yeah, correct. If we are doing this on let's say 28 or 32, 32 image, then we may see by naked eye.\n",
            "[01:32:09.140 --> 01:32:11.140]  Let me complete.\n",
            "[01:32:11.140 --> 01:32:15.140]  The problem is it is not, the computer can understand the mistakes.\n",
            "[01:32:15.140 --> 01:32:19.140]  Human eye will not because our vision system also do interpolation.\n",
            "[01:32:22.140 --> 01:32:24.140]  So that is the problem.\n",
            "[01:32:25.140 --> 01:32:31.140]  The neurons in the visual cortex or in the retina and then there is a behind the retina,\n",
            "[01:32:31.140 --> 01:32:34.140]  there are some neurons that actually sense the image and send it back.\n",
            "[01:32:34.140 --> 01:32:39.140]  They have their own activation and temporal time that it takes to activate and deactivate.\n",
            "[01:32:39.140 --> 01:32:42.140]  So it does naturally an interpolation.\n",
            "[01:32:42.140 --> 01:32:48.140]  That's why we see shapes in certain classic images, even if it is not explicitly drawn.\n",
            "[01:32:48.140 --> 01:32:50.140]  Our brain does all those interpolations.\n",
            "[01:32:50.140 --> 01:32:53.140]  So there is a limit up to which you want to achieve it.\n",
            "[01:32:53.140 --> 01:32:55.140]  Okay, sir, understood.\n",
            "[01:32:58.140 --> 01:33:00.140]  Shall we start?\n",
            "[01:33:00.140 --> 01:33:03.140]  Sir, one question actually.\n",
            "[01:33:03.140 --> 01:33:05.140]  Last question maybe.\n",
            "[01:33:05.140 --> 01:33:12.140]  In that bilinear interpolation, how do we get the parameter at which we want to pick up the value?\n",
            "[01:33:12.140 --> 01:33:17.140]  No, the UV value. Let's say UV is as I was trying to explain you.\n",
            "[01:33:17.140 --> 01:33:21.140]  UV is let's say it is somewhere not in the middle.\n",
            "[01:33:21.140 --> 01:33:23.140]  So the parameters are automatically coming.\n",
            "[01:33:23.140 --> 01:33:27.140]  If you really see here, the X component is U.\n",
            "[01:33:27.140 --> 01:33:34.140]  This 0.7, that means it is closer to this pixel than this pixel.\n",
            "[01:33:34.140 --> 01:33:40.140]  So it will have 0.7 of influence of this and 0.3 of this to get this value.\n",
            "[01:33:40.140 --> 01:33:45.140]  Similarly, it will see here that it is closer to the bottom pixel than the top pixel.\n",
            "[01:33:45.140 --> 01:33:50.140]  Then it will take appropriately 0.7 here and 0.3 here or 0.8 here and 0.2 here.\n",
            "[01:33:50.140 --> 01:33:54.140]  And then I'll get the new intensity value with those parameters.\n",
            "[01:33:54.140 --> 01:33:57.140]  So it depends on the decimal value.\n",
            "[01:34:01.140 --> 01:34:05.140]  And sir, can you please explain what is this Gaussian interpolation?\n",
            "[01:34:05.140 --> 01:34:08.140]  Gaussian interpolation is what we are doing in the blurring, that's all.\n",
            "[01:34:08.140 --> 01:34:13.140]  So all it says, I will take these four neighbors or I can take more than four neighbors also.\n",
            "[01:34:13.140 --> 01:34:16.140]  In that circle, I will take all neighbors.\n",
            "[01:34:16.140 --> 01:34:21.140]  Now intensity of this point will be more influenced by these four.\n",
            "[01:34:21.140 --> 01:34:25.140]  So when I'm doing averaging in blurring filter, this is essentially the same thing.\n",
            "[01:34:25.140 --> 01:34:33.140]  Instead of blurring, I am computing appearance of this point by taking into consideration appearance of all these pixels which are inside the circle.\n",
            "[01:34:33.140 --> 01:34:37.140]  Circle is more like representing the variance of the Gaussian.\n",
            "[01:34:37.140 --> 01:34:42.140]  We are outside this anyway, the importance of the contribution of pixels is 0.\n",
            "[01:34:42.140 --> 01:34:50.140]  So for these four, the center of the circle, the more influenced the appearance will have.\n",
            "[01:34:50.140 --> 01:34:55.140]  So we apply different weights to all these pixels inside the circle.\n",
            "[01:34:55.140 --> 01:34:58.140]  Yes, and add them weighted average.\n",
            "[01:34:58.140 --> 01:35:02.140]  The weights are Gaussian weights.\n",
            "[01:35:03.140 --> 01:35:07.140]  OK, so we'll stop here.\n",
            "[01:35:07.140 --> 01:35:14.140]  We'll continue to more complex transformation in the next class and then we'll take up the next topic on next Friday.\n",
            "[01:35:14.140 --> 01:35:17.140]  This is a small announcement.\n",
            "[01:35:17.140 --> 01:35:29.140]  So that we have to, I'm yet to get the TAs and without TAs, we cannot plan the project teaming and all, at least making team building exercise.\n",
            "[01:35:29.140 --> 01:35:31.140]  So.\n"
          ]
        }
      ],
      "source": [
        "!whisper \"class4.mp3\" --model medium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read article\n",
        "text_article = open(\"class3.txt\", \"r\").read()\n",
        "print(len(text_article.split()))\n",
        "text_article"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "EDNmDxURJm55",
        "outputId": "14e33358-31b5-47a1-f6fc-512d3900a03e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9783\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"So essentially last time we started looking at the spatial filtering for those who are\\nnot there and just quickly revise.\\nSo the idea is that we can put a filter mask, right, W here in this example three by three\\nand I can put it on every neighborhood of three by three on the image which is shown\\nhere in the zoomed version as the function, the 2D function, so x and y are the two domain\\ncoordinates.\\nSo we can have four neighborhoods, eight neighborhoods, whatever we look at it and then we can actually\\napply each element of this mask will be multiplied to the corresponding image element and then\\nI'll sum it up and whatever value I got that value I will divide by the in this case some\\nnormalizing factor which could be simply the sum of all weights of the mask and that value\\nI'll assign to the f of x, y or I will update f of x, y which is the center pixel with that\\nvalue, right.\\nSo this was we looked at it for each neighbor we are taking the corresponding weight in\\nthe mask multiplying it summing it over and appropriately normalize, okay.\\nSo you just put this mask at all locations, right.\\nSo if we take this mask of all once in the mask, right and then we take mask of different\\nsize as the size of the mask is increasing we can see that blurring is increasing, right.\\nSo the top left is three by three mask, top right bottom is 35 by 35 mask, right.\\nSo as the neighborhood is increasing we are getting blurring, it's also called smoothening\\nfilter, linear filter or a low pass filter.\\nWhy linear?\\nBecause we are simply doing a summation of all neighbors whatever is dictated by the\\nmask size and then simply replacing the center pixel with that average value, okay.\\nSum of all pixels divided by number of pixels and that's it, okay.\\nSo this is a mask one by nine into all once, so I'll apply this all once to all neighborhood\\nmultiply which equally basically means you take a three by three neighborhood around\\nsum all the intensity values divide this sum by nine and replace the center value with\\nthat value center pixel value with this averaged value, right.\\nSo it's also called low pass filter because all what is surviving is the low frequency\\ndetails which is structural details like even in the blurred image I can see approximately\\nthat A is written there and then some bars are there some squares are there but the sharpness\\nof the image is gone, the high frequency details so if you really see this area this is gone,\\nokay because all we see the blurred out area in this case and it is gradually blurred out\\nas you go from left to right and top to bottom.\\nSo this is a non-linear filter called Gaussian filter.\\nWhy non-linear?\\nBecause the formula that we were talking last time was actually a non-linear formula it\\nis exponential is involved there so this is a discrete mask of a 2D Gaussian filter so\\nwhat you are seeing here is the 43 in the middle is the peak and then we have the other\\nvalues are gradually decreasing similar to this peak in the middle and gradually we are\\ndecreasing in both the x and y dimension and we looked at how we increase the filter size\\nremains same but the parameter of the Gaussian which is the variance if you increase that\\nthen we get gradual more and more blurring in the image okay and this is just a visualization\\nof in 1D if you look at the variance as the variance is increasing you can see the blurring\\nin the image is increasing okay and we looked at some application of that to remove the\\nbackground noise and only get the mask of what are the foreground object which are significantly\\nlarge enough.\\nNow we will go to other kind of non-linear filters or so let's call Laplacian so before\\nwe go there let's try to revise some basics so what you are seeing here is d of d dx of\\nf is the first order derivative so any 1D function in this case discrete function which\\nis defined at finite values of x then I can basically take the very first order derivative\\nof that as f of x plus 1 minus f of x ideally if you remember the formula there is also\\nlimit involved into that you are higher secondary you might have studied it okay so that basically\\nmeans limit h tends to 0 f of x minus 1 or f of x plus h minus f of x divided by h right\\nthat is original formula which I am not writing here to avoid any confusion so this basically\\nmeans how function is changing as x is increasing okay so df by dx is derivative of f with respect\\nto x right that means how the function is changing as x is increasing okay or x is changing\\nby one unit so I can think of it as if there is a 1D function what is shown on the left\\nside which is think of it as a one line of the image so all I am plotting is the intensity\\nvalues at all pixels on one of the line right and this scan line we also call it scan line\\nthese are the intensity values in some range okay let's say this is between 0 to 25 I am just\\nshowing here between 0 to 10 so now you can see this is the in the array what I am showing is\\nthe intensity values and this is a plot of that okay now you can see the way intensity is changing\\nthere's a flat area all having six value then there is a ramp so there's a gradual decrease\\nin intensity so it is a kind of slow sloped edge you can say in intensity so you might say\\nthis is brighter and this is darker and then this is gray area in between and then there's a step\\narea where there's a sudden increase so right there's a sharp edge okay and then I have high\\ncontrast area again so then you can see this is how I can look at one pixel line also called\\nscan line in the image okay now if I take the first order derivative I can plot it below as\\nthis dotted line right so the dot here just look at the dot first don't look at the boxes so this\\nis the first order derivative and this is how it is written here also so how Sx is increasing so\\nsix minus six is zero six minus six is zero right six minus six again so depending on where I start\\nand then I get five minus six is minus one okay four minus six is again minus one and then I until\\nI reach two point I don't know why it is moving automatically with mouse issue or what so then I\\nget all zeros here right with corresponding to all flat area I get all zeros here and then again I\\nget a sharp edge so in the sharp edge six minus one is five right so this is the first order\\nderivative keep that in mind okay is that clear to everyone when I say first order derivative what\\ndo I mean am I online am I audible to everyone yes sir why are we doing here first derivative\\nand second derivatives any purpose sir yes I will come to that just have faith in why I am\\nexplaining this you will know that by end of this this description that I do in this slide\\nso this is fine\\nit's a good question I'll come to that this is fine sir at the point where it is transitioning\\nfrom two to one so one minus two should have been minus one yeah here but under one it is zero\\nno there is a shift here right so here if you see six minus one five so this is called derivative\\nat this point right okay now that how much is function changing in the next step and the\\ncurrent step okay yes yeah is that clear now we can also look at the second order derivative\\nso second order derivative you just take this and then you take another level of derivative\\nright so you you say d dx of df of dx which is also called dou square f by dou x square\\nokay so that is second order derivative that is also called Laplace okay it's an Laplace operator\\nand that's the first order derivative called gradient operator second order derivative\\ncalled Laplace operator and the name of the mathematician Laplace so that basically means\\nhow a function is changing across two port time points so you are saying that f of x plus one\\nplus f of x minus one minus two of f of x that's equivalent to take this array and then apply\\nfirst order derivative again that's all right so essentially this is what we will do so if you\\nsee that you will see that when I say that minus one and minus one so if I say minus one minus\\nminus of one that will be zero right so essentially I am just applying second order derivative either\\nif I apply this formula onto this array or if I apply first order derivative onto this array I'll\\nget this okay this basically means how the function is changing okay at the second level\\nsecond order change in function okay the first order derivative will tell me the change in\\nfunction values here the function is a pixel intensities on a particular row of the image\\nparticular line on the image okay so the second order derivative is here to you\\nyou take any element okay we can compute that okay so let's say three right so what will happen to\\nthree I will take right so I will take four I will take two four plus two is six because\\ntwo is f of x plus one for with respect to three here two is x plus one f of x plus one and four\\nis f of x minus one so four plus two is six right minus two times of three which is six so that will\\ngive you zero okay that's all now let's look at these plots at the bottom okay what does they\\ntry to capture so if you look at the dotted line only which is characterized by this\\nin circles blue circles you will see that the first order derivative if I just plot it it is zero zero\\nminus one right then minus one it's continuing then again it becomes zero zero right then it\\nbasically goes to five suddenly then it comes down to zero again and zero okay now if I look at the\\nsecond order derivatives what do I get sorry guys the second order derivative if you look at it\\nwhich is the boxes so somewhere it is overlapping in the beginning zero zero minus one but instead\\nof continuing to be zero it becomes sorry continuing to be minus one it becomes zero again\\nso zero zero zero then again there is a I'm setting you there with the mouse let me change\\nmy usage of that okay so then essentially I will get to all zeros then I will again get a plus one\\nhere then again I am getting these four zeros then I am getting a again plus five but the next step\\nis instead of being zero like in case of gradient first order gradient the Laplace is I hope this\\nis visible to you the Laplace is going to minus five so from plus five I am going to minus five\\nthere's a sharp change in second order derivative value Laplace value okay and then again I get to\\nzero now if you really look at what we are trying to capture here why we are interested to look at\\nthe derivatives and that question that was asked the second order derivative is capturing the\\nthe edges sharp increase in intensity value so I get a this kind of a zero crossing here what we\\ncall but suddenly I go from positive to negative this is where I am getting a sharp edge in the\\nintensity value while on the other hand when I have a ramp edge a slow ramp is there there's\\nintensity change but that is very gradual then I am essentially getting these two peaks right and\\nthese two peaks are essentially what I am getting as this here a small peak here and a small peak\\nhere right so there is no zero crossing immediate zero crossing right so essentially this second\\norder derivative even first order derivative captures the change in intensity values in this\\ncare along x-axis along x direction right so essentially I am looking at edges and how big is\\nthis magnitude of change in the second order derivative is capturing the how strong is the\\nedge is there is a very sharp edge or is there is a very gradual edge right so the big change is\\nindicating a sharp edge in the horizontal axis right so that means the edge is vertical you\\nknow think of that okay the intensity is changing in that area okay so I had a low intensity and\\nsuddenly I had a high intensity in one line if I assume the same line is replicated in the image\\nthen I see a vertical edge right now here that is also an edge but it is a slow edge it is a\\nslowly graduating gradually increasing intensity value so it's more blurred out edge than this edge\\nwhich is sharp edge is that clear why we are interested to look at this\\nlaplacian of the 1d intensity of a one row of the image because we want to basically look at\\nmathematically how do we define an edge for a computer like unlike human we can just see and\\ntell there is an edge for computers to understand we need to define something a mathematical\\noperator and then put some threshold on that and then recover the edges so this we are doing to do\\nbasic edge extraction from the image is this case clear to everyone one d if any questions\\nplease ask see there are no stupid questions okay all questions are important does that only\\nfew people have courage to ask so please ask the question now\\nsir what does can you tell again for derivative signifies change in intensity what is derivative\\nhow function is changing right as x is changing how f is changing f of x is changing right\\nso when i take the successive successive locations in this in this particular row of the image\\nwhat i am seeing is how intensity is changing right so there are two areas where intensity is\\nchanging from a high six it's going to zero or it's going to one sorry and then from one again\\nit's going to six okay now essentially what is happening here from when it goes to see six to one\\nit is slow the slope is there the intensity change is slower and here it is very sharp\\nright so if i just put six as a grayish value and one as a blackish value then i will see there is\\nan edge here right now this derivative is giving a high response on the edge first order derivative\\nsecond is giving even higher magnitude change across plus five to minus five that signifies\\nif i just look at the second order derivative i know there is an edge here and i will know that\\nthere is a gradual edge here some change of intensity start happening here and some is\\nhappening here right so then these areas are important in terms of change of intensity along\\nx direction right so when i say change of intensity around x that means there is a vertical edge\\nright this line is part of a vertical edge right line means a particular pixel\\nrow in the image this is a toy example we understanding this first is that clear\\nyes clear and coming on the first derivative sir when it is changing from one to six the value of\\nscan line right so from changing from one to six means six minus one is five it's okay but again\\nfrom six to six zero is okay second one zero minus five second derivative sir why this like\\nokay sir okay got it thanks got it yeah and if you go third order it will be same like\\nthird derivative it will be like the second derivative only like because\\nthe variations are not that high it seems to be third order we are not going there\\nokay sir okay now if you really look at the same second order derivative can be extended to\\n2d so now my function has two dimensional domain right x and y in the image i have row and column\\nokay i am it's an array 2d array right so i vary x i'll move along x and i'll also move along y\\nbecause edge need not to be only horizontal or vertical but i all i am doing is partial derivative\\nso this dough are called partial derivatives along x plus along y so this is a second order\\nderivative laplace along x along y so this total formula is representing second order derivative\\nalong x and y right of a 2d function f which is an image discrete image which is equivalent to say\\ni'm simply doing this f of x plus 1 comma y plus f of x minus 1 comma y minus 2 times f of x y this\\nis only when i keep very x and keep the y constant so this is the first partial derivative similarly\\nwhen i look at change in function value along y but i keep f x as constant so this is all this\\nformula means there's no need to get scared because when many years since you have looked at it maybe\\nany of you so this is a second order derivative this is a partial derivative along x second\\norder partial derivative along y and i just simply add them to to get the net derivative to at that\\npoint this is how the intensity is changing across left to right and this is across top to bottom\\nright now if you really look at i can rewrite if i just sum these two what will happen it will\\nlook like this f of x plus 1 plus f of x minus 1 comma y okay where y is constant then when x is\\nconstant minus minus 2 plus minus 2 is minus 4 times of f of x y which can be written like a mask here\\nisn't it for since this mask is defined for the center pixel f of x comma y what i am writing is\\nleft element which is f of x minus 1 comma y right element f of x plus 1 comma y and then\\ntop element f of x comma y plus 1 and bottom element f of x comma y minus 1 these fours\\nthe intensity of these four points i will add minus i will subtract four times the intensity\\nat a middle pixel and that will give me the gradient sorry laplacian second order gradient\\nright it's called laplacian\\nso this is a mask for 2d discrete laplacian\\nis this construction too clear to everyone when i apply this mask to any neighborhood in the image\\nall i am doing is this operation the bottom which is nothing but the this operation second\\norder derivative change in intensity values along x and y is this mask clear to everyone\\nthis is called laplacian filter spatial filter so basically when we will apply this filter on\\nour image so we are trying aged in image right yes okay we are trying to extract wherever there\\nis a intensity change right here a value of the pixels or brightness of the pixels right\\nin gray scale image this is just the brightness okay we are not talking about color images yet\\nokay any other question\\nthis mask is clear to you like gaussian mask we saw that was corresponding to a non-linear\\nfunction a discrete form of that this is this discrete form of a laplacian\\nwhich is second order derivative operator right sir so this filter will extract both\\nhorizontal and vertical edges yes okay it will try to find wherever there is an intensity gain\\nacross x and y why does the mass sum of value needs to be zero\\nbecause that's how the operator is below that you see here\\nokay so this should be resulting to zero so that's how no no it's not resulting to zero\\ndon't interpret that way there we come up with a theory theory is change in intensity across x\\nchange in intensity across y second order change right laplacian in along x along y when you write\\nit if i just replace these two terms in this formula i will get this right all i am doing is\\nputting that in a mask right right okay thank you sir in previous slide we came to know about the\\nedges change in edges okay along x along x okay through first order derivative and second\\norder derivative on the current one the next one we found the filter which is like mask\\nusing the second order derivative formula yes so what is the advantage\\nyeah i'll show you the advantage in next slide okay and once we have the derivative second\\norder derivatives if we given any image do we identify i'll just show you that okay you have\\na tendency to going fast forward which is fine because you should know the motivation\\nbut sometimes what happens is people many people struggle with the basics\\nright so i'm just defining first the mask now what is the impact of that we will see\\nokay now this is just the example of extending that mask in the middle to say i am also interested\\nto see the change in the function values across diagonals right so this is f of x comma y this is\\nf of x plus one and y minus one this is f of x minus one and y minus one this is f of x plus one\\nand y plus one so what we are saying is it is not just horizontal and vertical change we are also\\ninterested to see along these diagonals okay and this mask on the right side is just a flipped\\nversion of that by simply doing a multiplying all elements by negative right it will not change\\nanything it will not change the magnitude of the derivative right all the sign will change now\\nlet's look at what do we get if i apply this in the middle so the left image is the input image\\nif i apply this laplacian mask this is what i will get so now you can see i am getting all\\nthe sharp edge around the boundary of the moon right and then i am also getting this small small\\ncircles these edges are also i'm getting now interestingly if i add some component of that\\nc times this edge map to the original image i get a sharper image\\nright so essentially this is the edge extracted by that laplacian filter by applying it on the\\noriginal image so in the image if you see intensities it is capturing how it is varying\\nso these edges are getting around the boundary at every point applying this mask 3 by 3 mask i will\\nget this so this is giving me the edge responses right on all locations in the image and if i take\\non all locations in the image and if i take some c times of that sorry if i take c times of that\\nsome constant value depending on how much sharpness i want and if i add to the original image i will\\nget a sharp image so now these craters that are out there they're looking much sharper in this image\\nthan in this image that's because we are doing enhancement of the image or you can also say\\nrestoring the images because when i'm imaging there's some blurring that is happening because\\nof medium and all so i'm restoring the image with sharpness which more details which are which is\\ncoming from the image itself no other source i am adding i took the image i took its second\\norder derivative by applying the laplacian filter on all locations in the image i got this in the\\nmiddle so left is the input middle is the output of the laplace and then if i add these two with\\nsome constant multiplied to this i will get this image on the right so i hope the motive is clear\\nto you why we are interested to look at laplacian what is the output how it will look like and where\\nit will be used yes sir one question on this why are we using a laplacian filter is there any other\\nfilters to exclude one there are i'll talk about it have you understood this\\nwhy here to look at the construction of laplace right what we are trying to see we're trying to\\nfind change in intensity along x and y right and also diagonal also if you have that filter that\\ni have shown it all once around and eight in the middle minus eight in the middle right that means\\nfor each neighborhood at that point how is the intensity changing\\nokay so that basically will tell me if this particular pixel if it is in the flat region\\nwhat will happen if i take this mask here in the middle let's say a simpler this mask\\nand if i apply to flat region what will happen i will get zero\\nzero see if i take the intensity values are all 100 let's say so 100 multiplied to zero is zero\\nanyway these are zeros 100 plus 100 plus 100 plus 100 minus 400 isn't it yeah what it is it is zero\\nso flat reasons will give you zero response right now when there is a small intensity change i will\\nget small weak edges weak response when i get a large intensity change i will get a strong response\\nso the large intensity change let's assume that i have 200 here and i have 50 here right okay and\\nlet's say this is part of the edge so it is somewhere around 200 150 so what edge i will get\\nalong this axis right so this area will show a high edge response means this pixel is part of the edge\\nokay now if these two pixels and neighboring pixels are also giving a weaker response that\\nmeans this edge is basically a vertical edge because intensity here is higher intensity here\\nis lower same thing will happen on the neighboring pixels also\\nokay i'll give you one more attempt to connect guys because unless i draw\\nthis parity will continue and next time i'll try to test myself and fix it because\\ni just got hold of the laptop\\nlet me go on mute for a moment so that there's no echo unbearable echo for you\\nokay\\nam i audible\\nyes yes is there an echo\\nyes or no no sir no\\nokay so let me see if i can share my screen from this ipad also\\nyeah i can make notes definitely\\nso we have to stop sharing from here\\noh can't share the screen okay we'll leave it\\nand come back to what i was saying\\nso the screen is visible\\nyes sir\\nokay so we're here\\nvisible right\\nyes sir okay so\\nbasically if i apply that filter to a particular location around the boundary of this\\nmoon surface i will get this edge responses right and now i am just multiplying a part of it\\nsorry i'm just multiplying a part of it some component of that in terms of some c times of\\nthis to each element of the input image that is giving me sharpened image right\\nokay now this is an example where i show the sharpened image is better when i use this filter\\nthis laplace which also captures edges around the diagonals the one here is only capturing\\nedges along x and y and you can see the difference in sharpening right so here the sharpening is\\nhappening only mostly in the edges which are around x and y this is happening also diagonally\\nokay so it also depends for different implementation or different manifestation of laplace\\nfilter you can basically capture different quality of edges\\nis this here why this image is sharper so what do you see here\\nsome constant how do we determine this constant empirically you choose you want to see how much\\nsharpening you want sometime if you over sharpened then the image look bad also right\\nokay so so is it something a linear number like yeah some linear in some range zero to whatever\\nokay all right\\nis that clear everyone\\nokay so what we have achieved with laplacian we have computed edge map that edge map we are using\\nby adding to the original image to sharpen the image\\nright now there is one more way to look at it\\nsomebody masked if laplacian is the only way ultimately what they're interested is\\nto sharp the image so there's a technique called unsharp masking or high boost filtering\\nwhich says if this is a signal this is the 1d plot of the intensity along and\\nrow of the image right if i blur it it will look like this with simple gaussian filtering\\nyes or no look at the signal on the first plot it is the original intensity if i blur it if i\\naverage it then this is how the dotted is the original and the curve what i'm seeing is the\\nblurred so structure is preserved but the details are lost this sharpness in the intensity change\\nthese edges are lost now it becomes smooth curve\\nright just think of it if this was six right and or sorry this was all once and then i gradually\\nwent to six then if i average it then this will become close to five four three two here also\\nthis will become two and then it will become one so i'm basically blurring is that clear\\njust look at the first two top plots is this clear what do you mean by applying a smoothening\\nand or low pass filter and blur the function blur the signal\\nis that clear yes sir yes smoothening opposite to the\\ncontrast sharpening oh yeah yeah i am blurring okay now if i take the derivative of this smoothened\\nfunction right what we will i get i will get this kind of sorry i'll get this kind of plot\\nsee if this was a continuous i will get a very sharp change in intensity here right\\nzero everywhere else and suddenly sharp we have seen that when we are going from one to six\\nright you are getting a sudden change i'll go back to this slide and explain\\nyou see here when we are going from one six to one even though we are going gradually here\\nwe are getting a second order derivative was giving a sudden shift\\nright and then there is a constant change here minus one at every location because we are slowly\\nchanging the intensity here there was a sudden shift here when i go from one to six\\nright now what i am doing is if i blur out the function that sudden shift will be converted\\ninto this kind of lobes because now function is smooth the change is intensity is also a\\nGaussian now this is the impact of applying a Gaussian filter here if i apply Gaussian\\nfilter to signal and it takes in second order derivatives i will get this kind of peaks\\nright is that clear\\nis this clear\\nso why are we calling it as a high pass source\\nbecause what we are basically trying to i'll come to that okay so here it's a low pass filter\\nwe are basically smoothening the function and now i am trying to say i because i am interested in\\nedges i am interested in details the details are captured by second order derivative laplace\\nyes or no\\nyes sir right and edge response means it is a high pass all it's capturing is details\\nright a blurring is a low pass operation low frequency details are retained high are thrown\\nout high are suppressed while here there is no low frequency details we don't see the appearance\\nof the moon and all what we are seeing is only the sharp change in intensity which is high\\nfrequency details and we are adding this high frequency details to original signal to improve\\nits quality enhance it sharpen it right so here the idea of high pass means now i am applying this\\nkind of laplace kind of filter to only see where the sharp change in intensity you see the structure\\nof the signal is lost this is a structure which is somewhat preserved in low pass but here there\\nis no such structure all i am seeing is response towards this area and response towards this area\\nokay if i add this high pass details to this low pass filter i will get this kind of behavior\\nwhat does that mean i am increasing the sharpness of edges here the edge was the flat area and a\\nsmall increase continuous increase with some ramp right here there is a sharp decrease at that point\\nand then increase similarly here there is a sharp increase and then decrease so i am enhancing the\\nedges here so it's also called high boost filtering or unsharp masking wire first i am unsharping it\\nblurring it then i am computing its high pass filter applying to it and then i am adding this\\nto the low pass which basically gives me a reconstructed signal\\nso i am emphasizing i am increasing the magnitude of the original\\nedge boundaries i am sharpening the image this is how it looks so this is the original image\\nright this is the blurred version of that and this is the edge map of that blurred version\\nif i add this edge map to this image with some three times of this increased intensity\\ni will receive an image which is sharpened as compared to original\\nyou can see how the boundaries are the way s is written here or this edge\\nor even these small scratches and all they all are looking much more clear sharpened\\nhere we'll be adding this high intensity image to the original image right sir not the low pass\\nimage no we are adding it to blurred image low pass image you see here why because this if you\\nreally see if you add this to this image then it will look unnatural because there is no continuous\\nslope curve here right this is a discrete change sudden change right this is a continuous change\\nso adding a continuous function here to continuous function here will give me this kind of slopey\\ni can choose to add to this also ultimately see this anyway we are doing we are taking\\nthis and computing laplacian of this and adding to it before right now we are blurring this image\\ncomputing the edge map and then we are adding these two to get this\\nso what it is doing unlike before here if you really look at here what we were doing\\nwe were taking the original signal and only at the boundary areas we were adding details\\nwherever there is an edge we are adding change in intensity right here we are also saying that\\nwe are smoothening the function so that there is some so the areas which are flat will get some\\nsmoothness and the areas where we have the intensity change we are getting a sharp change\\nin intensity here which will look much more improved sharper sir can you please go back\\nto the previous slide the sir here in the right means the fourth plot we are saying that one plus\\nlambda of three we are not saying two plus lambda of three we are saying one plus lambda of three\\nright so are we not adding it with the original image lambda times three okay yeah maybe i\\nlet me look at what is that bottom i got confused because this is also a valid addition\\nwhat i am saying is also will give you a valid image it will actually it will also smooth around\\ndetails in the area of the flat area but here maybe you're right in the interpretation\\nthis is one plus three okay three times some lambda times three so lambda is the scaling factor\\nyeah i sorry my bad so what we are seeing at the bottom let me see that\\nso g of x is the smoothening so f of x plus y plus k time the mass this is the smoothening\\nand you are computing the gradient of g only if i'm correct it is f bar so g mask is okay i'm\\nsubtracting this derivative from here okay i'll clarify this in the next class i should quickly\\ncheck that been a long time that i have taught this part but what i am saying here also see\\nmy only worry is this is smooth right so if you add this you might so probably you're right because\\nthis area looks still flat even after adding this okay so that that that is because of this part\\nthis edge is also relatively flat in the middle so only this area where it is smooth and this area\\nthis is close to zero anyway yeah yeah so you're right it's being added to signal but i'll again\\nverify the formula and then get back to you but my only important point is this edge map is not\\ncoming from the original image it is coming from the bird blood image if i add to this to this this\\nis better than adding a direct edge map of this okay and that is because there is some kind of\\nsmoothening happening in this area if you really look at it this area there is some smoothening\\nhappening so now i am i am basically suppressing this high frequency details which are not so\\nimportant necessarily only highlighting in this area okay and then this edge map will add value\\nto this more than simply laplacian okay so this is another way to do unsharp masking so just\\nnote it down correctly that instead of adding it to blur the formula there and the figure there is\\ntalking about adding it to the original signal\\nokay there is another filter that will come called bilateral filter that's where\\nprobably i got confused with\\nokay so we we are we are adding the sharpening one to the original original signal okay in that\\ncase no problem but why are we calling this slide as unsharp masking because this slide was unsharp\\nmasking what we are doing is we are basically trying to first smooth the image blur the image\\nand then compute the edge and then then basically add it to one instead of computing laplace of\\none and adding to one we are computing blurred version of signal one to two then we are computing\\nit laplace and then we are adding it to the original signal one\\nsir can you explain the below term g mask that is what i will come back to this\\nsir one question so why if you see the image for graph four it is one plus lambda three right\\nwhy it is not a two plus lambda three it will take that is what now verified now we are adding\\nsignal and then this boosted edges to it this edge response is added to this so this flat area\\nremain flat this area also if you see reasonably flash ramp this area is also flat all we are\\nadding is in this area and this area sir one question so when we are getting the\\nlaplace transformation from the two so how it is different than getting the laplacian from the one\\ni mean how how difference it would make to that image this is a good point that is what i was\\ntrying to explain here if we look at the details here in the background right soil or whatever\\nwall it is right right now these details when i blur it the image these details become less\\nprominent right basically the low pass will retain the structural details\\nright the background noise it will try to blur it so that edge map that you will get will then\\nbe kind of only focused on these edges which are important in the image so the enhancement is\\nhappening only for the signal not for the noise okay okay okay i can also do one more thing if\\npossible i can also try to threshold it so for example this this variation that you are seeing\\nhere right if this function was not purely flat then you will see small small bumps here also right\\nokay what i am trying to say if all these pixels neighboring pixels on one line do not have exact\\nsame intensity let's say this is 0.8 0.9 right 1.1 and i don't know why it's keep moving\\nright so if this area is having some small bumps here in the intensity change\\nthat will be also be part of this enhancement so one possibility is also i can say threshold this\\nimage and only retain the high change in intensity all the variants are possible okay so essentially\\nwhat is happening with blurring is those small changes i am actually ignoring and i am only\\nlooking at the structural details where there is a change in intensity at the largely and those areas\\ni am extracting and then i'm adding to original signal only to enhance that okay\\nso this was the original this is the sharpened image\\nagain not full proof right because there are still noise in this area and that is this is\\nbeing highlighted here if you see this is enhanced but look image look much sharper to us\\nit's another example of that\\nnow there are other spatial filters also so it's not like\\nfor edge extraction only Laplace is a solution then we also call a very simpler one which is\\nfirst-order gradient along the diagonal edges called robot cross gradient operator so it's\\na two by two filter okay so essentially you can the problem is localizing this filter so this\\nfilter came earlier than Laplace then then the Sobel filter is another filter which is giving\\nmore importance to gradients along x or along y right in the middle it's more side it's less\\nso some kind of a variants of you know feeling of Gaussian that is trying to give more importance\\nto horizontal or vertical edges and applying them appropriately will get different edge map\\nif you look at this is the Sobel operator applied so only edges across this diagonal are highlighted\\nso this edge is highlighted right you are getting sharper response in this area this area\\nif you look at the other opposite robot cross then this area is getting weaker response this\\nis getting sharper response so I can also apply this and this and add the two that will also give\\nme sharp edges across all the four two diagonal directions the Sobel on the other hand if you\\nrelate it it is basically giving higher response to the horizontal vertical edges if you see\\nchange in intensity along x these are the vertical edges are much more sharper than the\\ndiagonal edges\\nokay similarly here this is the response for horizontal edges right so wherever there is a\\nhorizontal change in intensity like this area you are getting a stronger response here\\na vertical you are getting a weaker response right vertical this particular area the hairs\\nyou are getting much stronger response in this Sobel operator than here\\nessentially I can design this kind of spatial filters people have done that\\nto extract different orientation edges which are oriented differently okay\\nso but ultimate idea I have to grasp that we are applying we are exploiting the idea of\\ncoherence in the neighborhood and change in intensity and applying appropriate mask filter\\nmask I am actually extracting edges and then I can use these edges for image sharpening kind of task\\nis this clear till here the spatial filter what we covered till now\\nany question on the Sobel any particular reason for writing plus one or minus one like using\\nsign especially plus sign that's a gradient huh no no that's true sir but earlier like\\nwhenever a positive number we were not using this plus sign just to emphasize okay okay got it\\nand second sir like first time we are able to see the numeral two minus two or plus two\\nso scaling giving more importance to horizontal edges in this case got it right more than the\\ndiagonal edges basically it can be plus three also it can be any other number also right\\nbut typically you don't go beyond that because that basically means\\nyou already get good sharpened response in that direction right\\nso here if you see the zero in the row right that means I don't bother much about the vertical edges\\nI am more interested in extracting horizontal edges or diagonal edges but horizontal should\\nbe more emphasized so that's why the vertical edges are neglected here\\nwhy in this case it's focused on vertical edges more the vertical edges are highly\\nemphasized right the diagonals are pretty much same in both\\nbecause I am doing plus one to minus one change so basically we will focus on negative values\\nwhich will indicate the sharpness in which either negative values what is the negative value this\\nis change in intensity right we look at the magnitude of that this is a mask I'll apply\\nthat mask on the image right and then I will threshold that image or I can plot it in the\\ngray scale also threshold I mean I can make it binary so zero means flat area right each pixel\\nwill be white or black for an edge map is it part of edge or no that's all output will be\\nthat's what is plotted here right or I can keep it gray scale but then I'll appropriately scale that\\nright so that's the whole idea\\nthis is an operator you have to keep that in mind I am applying it at every neighborhood\\nthis is a mask I'll put it in the image at every location and then I'll calculate this\\nmultiply minus one to the corresponding pixel here zero to the next pixel here and for this\\npixel in the center I am computing this change in intensity along x so two times the intensity\\nof the right neighbor minus two times the intensity of left neighbor plus intensity of the diagonal\\nright neighbor minus intensity of diagonal left neighbor all that I can do and the final value\\nis what I will put it here in the middle okay and then I will appropriately threshold it\\nto get binary or scale it to get the gray scale image which will be an edge map we call it edge map\\nsir I have a question sir yes sir on this slide we are able to see the output which is like a\\nvertical and the horizontal ones clearly after applying the mask three by three now if I don't\\nhave the output how do I know that mask or filter is horizontal one or vertical one simply can you\\nplease explain can you repeat your question without seeing the output if I see a grid with\\nthree by three how do I come to know this is a horizontal mask or vertical mask\\nbecause this is how they're defining it isn't it if you really look at it\\nzeros in the vertical in the middle that means I am not bothered about change in intensity from\\ntop to bottom when change in intensity apart from top to bottom happens that is the horizontal edge\\nI am more focused on change in intensity from right to left or left to right right\\nthat basically means I'm looking at vertical edge\\nso how you define the mask you will get the edge map\\nsir you are not interpreting the mask you are designing the mask you are interpreting the output\\nyes sir interpreting the output is correct yeah so how you design the mask may give you the output\\nyes sir the three by three mask which is zero zero zero having vertical\\nbasically here we are not able to see any changes in vertically so it is more yeah more horizontal\\nno change in vertically that basically means we are not looking at the horizontal edges you\\ndon't understand that yeah yeah okay but again it is not completely missing them why because you\\nhave this right yeah we have minus one along with it yeah so if I see the top row it goes from\\nminus one to zero to plus one it means that that that is why horizontal edges are not completely\\nlost they are weak okay now I'll just quickly look at some of the other filters spatial filters so\\nthis is the one which is called non-linear spatial filter why non-linear because in linear we were\\ntrying to take some fixed linear value multiplying and adding here the idea is to see if this is\\ncalled us some kind of a salt or pepper noise so basically we say that there is an IC and because\\nof some reason in the sensing we got this kind of dots which is basically sudden change in intensity\\nwhich is not consistent with the neighborhood so this area is relatively flat great but I am\\nseeing this intensity change so what I will do I take that neighborhood and only retain the\\nmaximum value or minimum value or or some kind of median value so this is a max filter so I'll take\\na three by three neighborhood around a pixel and I'll retain the maximum value in that area\\nso that will get away with this dark speckle noise okay if the noise is of other kind which\\nis all bright then I will keep minimum value so these are called max filter min filter right\\nthen if I don't know if the if the noises of type salt salt is this white color pepper is this black\\ncolor right and if I have a combination the two then I might go for other variants like median filter\\nfilter so then I will say I would like to keep only the median value in that particular three\\nby three or five by five mass what is the median value that median value I will give to the original\\ninstead of original pixel I will give so even this kind of image I can recover this kind of image\\nby applying median filtering\\nso these are also known as statistical filters order statistic filter\\nright we're looking at min max median and there are other variants of that also\\nokay this should be easy any question\\nmin max filter is clear to everyone\\nso previously we were adding the mass was multiplied element wise and summed and averaged\\nor whatever divided by some normalizing factor here we are simply for a particular three by\\nthree neighborhood out of nine values we are just retaining the maximum value selecting the\\nmaximum value and that intensity I'm copying in the middle filter middle pixel it's called max\\nfilter for pepper noise this is a max filter for salt noise and a median for salt and pepper noise\\nany question max for paper mean for salt right sir max for pepper mean min for salt\\nyeah\\nso what is the original signals are here the left one signal is on the left okay the middle one\\nis the noisy that we recover so the thing is this is what is you can think of that this is what you\\nget because of some sensing noise right so this is what you can recover after removing this you\\nhave to compare left and right left is the ground truth right is the predicted or filtered\\nor predicted filtered and the middle is going to us is noisy input\\nyou can see this is also not hundred percent you know you can see if this area for example in the\\neyes some salt noise is retained even in this area\\nright but then you got largely the signal out right you can see clearly more or less that this\\nis what the see no image processing method is full proof whether it's a noise removal or whether\\nit's whatever sharpening\\nokay but then you are trying to recover the signal and throw away the noise\\nand here we are also exploiting that we have a prior knowledge of what kind of noise it is\\ni cannot blindly apply min or max filter i need to know what kind of noise it is\\nokay so next class we'll do bilateral filtering and that's the last topic of this special filter\\nokay for image restoration\\ncan we stop here sir one question like\\nshall we stop here sir one question like\\nis is noise always of a high frequency typically it's a very good question\\nso right it's like when you're talking to somebody and you're saying hello\\nand because of the noise in the communication channel right you are getting the word hell\\nright that basically means oh is lost right but you are still calling it hello you can see\\nof that if you just leave the call and no other guy will not say hell to you\\nright and hell does not come without any context unless you really know who is calling you know\\nthe prior is it go to hell what it is right so now there's a probability that it could happen that\\nsomebody is calling hell while calling you and then you're hearing it as a hello\\nhello so the structural noise when i say high frequency details are not structural\\ndetails right so structural noise fundamental if a is written as oh right then it's a structural noise\\nbut then how can we differentiate like it was a and not oh that is the philosophical question\\nthen you need to start looking at what is noise and what is signal\\nright you never know if oh is written intentionally\\nright if somebody is uttering hell okay so then it's up to some after point of time the\\ndefinition of signal idea there's no way to decipher that\\nso this is a philosophical question of what is noise what is signal right you have to really\\nlook at some time for example sinusoids are having that kind of high frequency details largely\\nright you cannot call this noise also always typically noise is high frequency\\nand if it is not is it is a low frequency noise then you typically have a prior model of that\\nfor example in an image if there is a flyer that is captured or some background object is captured\\nand you want to remove it then it is something basically give a prior and tell okay this is the\\nbackground and remove it right you have to explicitly say this is noise\\notherwise system has no way of judging what is noise what is signal\\notherwise system has no way of judging what is noise what is signal\\nan image if you show to a human in a garden image human might see\\nbackground as a noise right in portrait image okay the grass is noise for an animal grass\\nhuman is a noise if they see the green lush grass they want to eat for them human is a noise\\nso it depends on what is the interest what we want to achieve\\nokay i'll just stop here because i have other commitments and we started late so i completed\\na bit late today and yeah i have one question so diagonal horizontal and vertical filters\\nthose doesn't have any mathematical way to actually create them right it's up to the\\ndesigner to create which one these uh yes yes sir these this is gradient right these are gradients\\nokay first order this is also odd scale gradient okay these are first order derivatives right\\nunderstood sir okay problem is this sorry no problem okay we'll start from here\\nokay next class thank you okay sir bye sir thank you thank you thank you sir\\nsomebody was still in waiting okay great\\nokay join on time okay today i was a bit late but otherwise i'm on time so otherwise i can't see\\nif somebody is waiting and admitting okay thank you and i will not release the slides guys i'll\\nonly release the video so don't request for that recordings will be released\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtbODuevJ61c",
        "outputId": "eaccc3e8-78c2-4b04-ee33-d560f45b5bee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n",
        "\n",
        "summarizer = pipeline(\"summarization\", \"google/pegasus-xsum\")\n",
        "tokenizer_kwargs = {'truncation':True,'max_length':512}\n",
        "text_summerization = summarizer(text_article, min_length=30, do_sample=False,**tokenizer_kwargs)\n",
        "\n",
        "print(text_summerization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWtLU3P1JiXi",
        "outputId": "63ee04b3-3033-47ef-db61-ebfbf4a069fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': \"In this lesson we're going to be looking at the spatial filtering for those who are not there and then we'll be looking at the low pass filter and then we'll be looking at the smoothing filter and then we'll be looking at the linear filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter and then we'll be looking at the smoothing filter.\"}]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN10aXOhHI214iz99CUbo/h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}